# === –ò–ú–ü–û–†–¢ –ò –ò–ù–ò–¶–ò–ê–õ–ò–ó–ê–¶–ò–Ø ===
import sys, os, warnings, gc, json, pickle
from datetime import datetime
from pathlib import Path
import torch, torch.nn as nn, torch.nn.functional as F
from torch.amp import autocast
import numpy as np
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
import seaborn as sns
from PIL import Image
from scipy import stats, ndimage
from sklearn.metrics import accuracy_score
from tqdm import tqdm
from diffusers import UNet2DModel, DDPMScheduler
from torchvision import transforms, models

warnings.filterwarnings('ignore')

# –ü—Ä–æ–≤–µ—Ä–∫–∞ Grad-CAM
try:
    from pytorch_grad_cam import GradCAM
    from pytorch_grad_cam.utils.model_targets import RawScoresOutputTarget
    from pytorch_grad_cam.utils.image import show_cam_on_image
    GRADCAM_AVAILABLE = True
    print('‚úÖ Grad-CAM –¥–æ—Å—Ç—É–ø–µ–Ω')
except ImportError:
    print('‚ùå Grad-CAM –Ω–µ –Ω–∞–π–¥–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install grad-cam')
    GRADCAM_AVAILABLE = False

# –ü—Ä–æ–≤–µ—Ä–∫–∞ XAI –±–∏–±–ª–∏–æ—Ç–µ–∫
try:
    from captum.attr import IntegratedGradients, GradientShap
    CAPTUM_AVAILABLE = True
    print('‚úÖ Captum –¥–æ—Å—Ç—É–ø–µ–Ω')
except ImportError:
    print('‚ö†Ô∏è  Captum –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω')
    CAPTUM_AVAILABLE = False

try:
    import shap
    SHAP_AVAILABLE = True
    print('‚úÖ SHAP –¥–æ—Å—Ç—É–ø–µ–Ω')
except ImportError:
    print('‚ö†Ô∏è  SHAP –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω')
    SHAP_AVAILABLE = False

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞
if torch.cuda.is_available():
    device = torch.device('cuda:0')
    print(f'üöÄ GPU: {torch.cuda.get_device_name(device)}')
else:
    device = torch.device('cpu')
    print('üíª CPU')

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ matplotlib
plt.rcParams['figure.figsize'] = (12, 8)
plt.rcParams['font.size'] = 11

print('üéØ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞!')

# –û—Å–Ω–æ–≤–Ω—ã–µ –∏–º–ø–æ—Ä—Ç—ã
import os
import warnings
import gc
from datetime import datetime
from pathlib import Path
import json
import pickle

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.amp import autocast

import numpy as np
import matplotlib.pyplot as plt

#Grad-Cam
from pytorch_grad_cam import GradCAM
from pytorch_grad_cam.utils.image import show_cam_on_image
from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget
import torch.nn.functional as F
from skimage.transform import resize

import seaborn as sns
from PIL import Image

# –ù–∞—É—á–Ω—ã–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏
from scipy import stats, ndimage
from sklearn.metrics import accuracy_score, classification_report
from tqdm import tqdm

# –î–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ (—Å–æ–≤—Ä–µ–º–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è)
from diffusers import UNet2DModel, DDPMScheduler
from torchvision import transforms, models

# –ü–æ–¥–∞–≤–ª–µ–Ω–∏–µ –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–π
warnings.filterwarnings('ignore', category=FutureWarning)
warnings.filterwarnings('ignore', category=UserWarning)

# XAI –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏
XAI_AVAILABLE = False
CAPTUM_AVAILABLE = False
SHAP_AVAILABLE = False

try:
    from captum.attr import IntegratedGradients, GradientShap
    from captum.attr import visualization as viz
    CAPTUM_AVAILABLE = True
    print("‚úÖ Captum –¥–æ—Å—Ç—É–ø–µ–Ω")
except ImportError:
    print("‚ö†Ô∏è  Captum –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install captum")

try:
    import shap
    SHAP_AVAILABLE = True
    print("‚úÖ SHAP –¥–æ—Å—Ç—É–ø–µ–Ω")
except ImportError:
    print("‚ö†Ô∏è  SHAP –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install shap")

XAI_AVAILABLE = CAPTUM_AVAILABLE or SHAP_AVAILABLE

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ matplotlib –¥–ª—è –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –≥—Ä–∞—Ñ–∏–∫–æ–≤
plt.style.use('default')
plt.rcParams['figure.figsize'] = (12, 8)
plt.rcParams['font.size'] = 11
plt.rcParams['axes.grid'] = True
plt.rcParams['grid.alpha'] = 0.3

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞ (–∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–æ –ø–æ–¥ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Å–∏—Å—Ç–µ–º—ã)
if torch.cuda.is_available():
    # –ü–æ–ø—Ä–æ–±—É–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —É–∫–∞–∑–∞–Ω–Ω–æ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
    device = torch.device('cuda:1' if torch.cuda.device_count() > 1 else 'cuda:0')
    print(f"üöÄ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è GPU: {device} ({torch.cuda.get_device_name(device)})")
elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
    device = torch.device('mps')
    print("üçé –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è Apple MPS")
else:
    device = torch.device('cpu')
    print("üíª –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è CPU")

# –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–∞–º—è—Ç–∏ –¥–ª—è —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö GPU
if device.type == 'cuda':
    torch.backends.cudnn.benchmark = True
    torch.backends.cuda.matmul.allow_tf32 = True
    torch.backends.cudnn.allow_tf32 = True
    
    # –û—Å–≤–æ–±–æ–∂–¥–µ–Ω–∏–µ –∫—ç—à–∞
    torch.cuda.empty_cache()
    gc.collect()

print(f"üî¨ XAI –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç—å: {XAI_AVAILABLE}")
print(f"üìä Captum: {CAPTUM_AVAILABLE}, SHAP: {SHAP_AVAILABLE}")
print("‚úÖ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")

# –£—Ç–∏–ª–∏—Ç–∞: —Ç–µ–∫—Å—Ç–æ–≤—ã–π –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä –≤ –ª–æ–≥
def _log_progress_bar(label: str, current: int, total: int, width: int = 30):
    try:
        current = max(0, int(current))
        total = max(1, int(total))
        filled = int(width * current / total)
        bar = '#' * filled + '-' * (width - filled)
        pct = 100.0 * current / total
        print(f"{label}: [{bar}] {current}/{total} ({pct:.0f}%)", flush=True)
    except Exception:
        # –ù–∞ –≤—Å—è–∫–∏–π —Å–ª—É—á–∞–π –Ω–µ –ª–æ–º–∞–µ–º –ø–∞–π–ø–ª–∞–π–Ω –∏–∑-–∑–∞ –ª–æ–≥–æ–≤
        pass

# === –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø –ü–†–û–ï–ö–¢–ê ===

# –ü—É—Ç–∏ –∫ –¥–∞–Ω–Ω—ã–º –∏ –º–æ–¥–µ–ª—è–º (–∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω—ã –ø–æ–¥ –≤–∞—à—É —Å—Ç—Ä—É–∫—Ç—É—Ä—É)
PROJECT_ROOT = Path(".").resolve()
MODELS_DIR = PROJECT_ROOT / "checkpoints"  # –ø–µ—Ä–µ–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–æ –Ω–∞ checkpoints
CHECKPOINTS_DIR = PROJECT_ROOT / "checkpoints"
DATA_DIR = PROJECT_ROOT / "data" / "ISIC2018_Task3_Training_Input"
RESULTS_DIR = PROJECT_ROOT / "xai_results"

# –°–æ–∑–¥–∞–µ–º –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
RESULTS_DIR.mkdir(exist_ok=True)

# === –ü–ê–†–ê–ú–ï–¢–†–´ –ú–û–î–ï–õ–ï–ô ===

# DDPM –ø–∞—Ä–∞–º–µ—Ç—Ä—ã (–∏–∑ –≤–∞—à–µ–≥–æ –∫–æ–¥–∞ –æ–±—É—á–µ–Ω–∏—è –¥–∏—Ñ—Ñ—É–∑–∏–∏)
DDPM_IMAGE_SIZE = 128  # –†–∞–∑–º–µ—Ä –¥–ª—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
DDPM_CHANNELS = 3
DDPM_TIMESTEPS = 1000
DDPM_BETA_SCHEDULE = "squaredcos_cap_v2"

# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞ (–∏–∑ –≤–∞—à–µ–≥–æ –∫–æ–¥–∞ –æ–±—É—á–µ–Ω–∏—è CNN)
CLASSIFIER_IMAGE_SIZE = 224  # –†–∞–∑–º–µ—Ä –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞
CLASSIFIER_BATCH_SIZE = 16

# –ö–ª–∞—Å—Å—ã ISIC2018 (–∫–æ—Ä—Ä–µ–∫—Ç–Ω–∞—è –≤–µ—Ä—Å–∏—è)
CLASS_NAMES = ["MEL", "NV", "BCC", "AKIEC", "BKL", "DF", "VASC"]
NUM_CLASSES = len(CLASS_NAMES)

# === –ü–£–¢–ò –ö –ú–û–î–ï–õ–Ø–ú ===

# DDPM –º–æ–¥–µ–ª–∏ (–≤ checkpoints/)
DDPM_MODELS = {
    'MEL': 'unet_MEL_best.pth',
    'NV': 'unet_NV_best.pth',
    'BCC': 'unet_BCC_best.pth',
    'AKIEC': 'unet_AKIEC_best.pth',
    'BKL': 'unet_BKL_best.pth',
    'DF': 'unet_DF_best.pth',
    'VASC': 'unet_VASC_best.pth'
}

# –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä
CLASSIFIER_PATH = CHECKPOINTS_DIR / "classifier.pth"

# –ò–º–ø–æ—Ä—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö –º–æ–¥—É–ª–µ–π (—Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫)
try:
    # –î–æ–±–∞–≤–ª—è–µ–º models/ –≤ –ø—É—Ç—å
    import sys
    sys.path.append(str(MODELS_DIR))
    
    # –ü–æ–ø—ã—Ç–∫–∞ –∏–º–ø–æ—Ä—Ç–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–≥–æ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞
    from melanoma_classifier import MelanomaClassifier as UserMelanomaClassifier
    USER_CLASSIFIER_AVAILABLE = True
    print("‚úÖ –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π MelanomaClassifier –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω")
except ImportError as e:
    print(f"‚ö†Ô∏è  –ù–µ —É–¥–∞–ª–æ—Å—å –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä: {e}")
    print("   –ë—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∞ –≤—Å—Ç—Ä–æ–µ–Ω–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è")
    USER_CLASSIFIER_AVAILABLE = False

# === –ü–ê–†–ê–ú–ï–¢–†–´ XAI –ê–ù–ê–õ–ò–ó–ê ===

# –î–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω–∞—è —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏—è
INFERENCE_STEPS = 50
SAVE_EVERY_N_STEPS = 5
GENERATION_SEED = 42

# XAI –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
TOP_K_PERCENT = 10  # –ü—Ä–æ—Ü–µ–Ω—Ç –Ω–∞–∏–±–æ–ª–µ–µ –≤–∞–∂–Ω—ã—Ö —Ä–µ–≥–∏–æ–Ω–æ–≤
BOTTOM_K_PERCENT = 10  # –ü—Ä–æ—Ü–µ–Ω—Ç –Ω–∞–∏–º–µ–Ω–µ–µ –≤–∞–∂–Ω—ã—Ö —Ä–µ–≥–∏–æ–Ω–æ–≤
IG_N_STEPS = 50  # –®–∞–≥–∏ –¥–ª—è Integrated Gradients
SHAP_N_SAMPLES = 512  # –°—ç–º–ø–ª—ã –¥–ª—è SHAP –∞–ø–ø—Ä–æ–∫—Å–∏–º–∞—Ü–∏–∏

# –ü–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ —á–µ—Ä–µ–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è
try:
    _env_save_every = int(os.environ.get("XAI_SAVE_EVERY_N", str(SAVE_EVERY_N_STEPS)))
    if _env_save_every > 0:
        SAVE_EVERY_N_STEPS = _env_save_every
except Exception:
    pass

try:
    _env_inf_steps = int(os.environ.get("XAI_INFERENCE_STEPS", str(INFERENCE_STEPS)))
    if _env_inf_steps > 0:
        INFERENCE_STEPS = _env_inf_steps
except Exception:
    pass

try:
    _env_seed = int(os.environ.get("XAI_GENERATION_SEED", str(GENERATION_SEED)))
    GENERATION_SEED = _env_seed
except Exception:
    pass

# –ò–Ω—Ç–µ—Ä–≤–µ–Ω—Ü–∏–∏
INTERVENTION_TYPES = ['blur']
NOISE_STD = 0.5
BLUR_KERNEL_SIZE = 5

# –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
ALPHA_LEVEL = 0.1
N_BOOTSTRAP = 1000
N_PERMUTATIONS = 10000

# –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
print("üìã === –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø –ü–†–û–ï–ö–¢–ê ===")
print(f"üìÇ –ö–æ—Ä–Ω–µ–≤–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {PROJECT_ROOT}")
print(f"üèóÔ∏è  –ú–æ–¥–µ–ª–∏: {MODELS_DIR}")
print(f"üíæ –ß–µ–∫–ø–æ–∏–Ω—Ç—ã: {CHECKPOINTS_DIR}")
print(f"üìä –î–∞–Ω–Ω—ã–µ: {DATA_DIR}")
print(f"üìà –†–µ–∑—É–ª—å—Ç–∞—Ç—ã: {RESULTS_DIR}")
print(f"üéØ –ö–ª–∞—Å—Å—ã ({NUM_CLASSES}): {', '.join(CLASS_NAMES)}")
print(f"üñºÔ∏è  –†–∞–∑–º–µ—Ä—ã: DDPM={DDPM_IMAGE_SIZE}px, Classifier={CLASSIFIER_IMAGE_SIZE}px")
print(f"‚öôÔ∏è  –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}")
print(f"üß† –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä: {USER_CLASSIFIER_AVAILABLE}")

# –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏—è —Ñ–∞–π–ª–æ–≤
print("üîç === –ü–†–û–í–ï–†–ö–ê –§–ê–ô–õ–û–í –ú–û–î–ï–õ–ò ===")

# –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä
classifier_exists = CLASSIFIER_PATH.exists()
print(f"üè• –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä: {'‚úÖ' if classifier_exists else '‚ùå'} {CLASSIFIER_PATH.name}")

# DDPM –º–æ–¥–µ–ª–∏
existing_ddmp_models = []
for class_name, model_file in DDPM_MODELS.items():
    model_path = CHECKPOINTS_DIR / model_file
    exists = model_path.exists()
    print(f"üß¨ {class_name}: {'‚úÖ' if exists else '‚ùå'} {model_file}")
    if exists:
        existing_ddmp_models.append(class_name)

print(f"‚úÖ –î–æ—Å—Ç—É–ø–Ω—ã DDPM –º–æ–¥–µ–ª–∏ –¥–ª—è –∫–ª–∞—Å—Å–æ–≤: {', '.join(existing_ddmp_models) if existing_ddmp_models else '–ù–µ—Ç'}")

if not existing_ddmp_models:
    print("‚ö†Ô∏è  –ü–†–ï–î–£–ü–†–ï–ñ–î–ï–ù–ò–ï: –ù–µ –Ω–∞–π–¥–µ–Ω–æ –Ω–∏ –æ–¥–Ω–æ–π DDPM –º–æ–¥–µ–ª–∏!")
    print("   –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ —Ñ–∞–π–ª—ã –Ω–∞—Ö–æ–¥—è—Ç—Å—è –≤ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏.")
elif not classifier_exists:
    print("‚ö†Ô∏è  –ü–†–ï–î–£–ü–†–ï–ñ–î–ï–ù–ò–ï: –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –Ω–µ –Ω–∞–π–¥–µ–Ω!")
    print("   XAI –∞–Ω–∞–ª–∏–∑ –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å.")
else:
    print("üöÄ –í—Å–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ —Ñ–∞–π–ª—ã –Ω–∞–π–¥–µ–Ω—ã! –ì–æ—Ç–æ–≤ –∫ XAI –∞–Ω–∞–ª–∏–∑—É.")

def create_ddpm_unet():
    """
    –°–æ–∑–¥–∞–µ—Ç UNet2DModel —Å —Ç–æ—á–Ω–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π –∏–∑ –≤–∞—à–µ–≥–æ –∫–æ–¥–∞ –æ–±—É—á–µ–Ω–∏—è
    
    Returns:
        UNet2DModel: –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –º–æ–¥–µ–ª—å
    """
    return UNet2DModel(
        sample_size=DDPM_IMAGE_SIZE,
        in_channels=DDPM_CHANNELS,
        out_channels=DDPM_CHANNELS,
        layers_per_block=2,
        block_out_channels=(64, 128, 256, 256),
        down_block_types=(
            "DownBlock2D",
            "DownBlock2D",
            "AttnDownBlock2D",
            "DownBlock2D"
        ),
        up_block_types=(
            "UpBlock2D",
            "AttnUpBlock2D",
            "UpBlock2D",
            "UpBlock2D"
        ),
        class_embed_type=None,
    )


def create_ddpm_scheduler():
    """
    –°–æ–∑–¥–∞–µ—Ç DDPMScheduler —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –∏–∑ –≤–∞—à–µ–≥–æ –∫–æ–¥–∞ –æ–±—É—á–µ–Ω–∏—è
    
    Returns:
        DDPMScheduler: –ù–∞—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π scheduler
    """
    return DDPMScheduler(
        num_train_timesteps=DDPM_TIMESTEPS,
        beta_schedule=DDPM_BETA_SCHEDULE,
        prediction_type="epsilon"  
        
    )


class MelanomaClassifierAdaptive(nn.Module):
    """
    –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –º–µ–ª–∞–Ω–æ–º—ã
    
    –°–æ–≤–º–µ—Å—Ç–∏–º —Å —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞–º–∏ –∏ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∞–¥–∞–ø—Ç–∏—Ä—É–µ—Ç—Å—è
    –∫ –≤–∞—à–µ–º—É –æ–±—É—á–µ–Ω–Ω–æ–º—É –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä—É
    """
    
    def __init__(self, num_classes=NUM_CLASSES, architecture='auto', pretrained=True):
        super().__init__()
        
        self.num_classes = num_classes
        self.architecture = architecture
        
        # –ü—ã—Ç–∞–µ–º—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä
        if USER_CLASSIFIER_AVAILABLE and architecture == 'auto':
            try:
                # –°–æ–∑–¥–∞–µ–º —ç–∫–∑–µ–º–ø–ª—è—Ä –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–≥–æ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞
                self.model = UserMelanomaClassifier(pretrained=pretrained)
                self.architecture = 'user_model'
                print("‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π MelanomaClassifier")
            except Exception as e:
                print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–π –º–æ–¥–µ–ª–∏: {e}")
                print("   –ü–µ—Ä–µ–∫–ª—é—á–∞–µ–º—Å—è –Ω–∞ –≤—Å—Ç—Ä–æ–µ–Ω–Ω—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É")
                self._create_builtin_model(pretrained)
        else:
            self._create_builtin_model(pretrained)
    
    def _create_builtin_model(self, pretrained=True):
        """–°–æ–∑–¥–∞–µ—Ç –≤—Å—Ç—Ä–æ–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å –Ω–∞ –æ—Å–Ω–æ–≤–µ ResNet18"""
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º ResNet18 –∫–∞–∫ –±–∞–∑–æ–≤—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É (–ø–æ–ø—É–ª—è—Ä–Ω–æ –¥–ª—è –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π)
        self.model = models.resnet18(weights="IMAGENET1K_V1" if pretrained else None)

        
        # –ó–∞–º–µ–Ω—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π —Å–ª–æ–π
        num_features = self.model.fc.in_features
        self.model.fc = nn.Linear(num_features, self.num_classes)
        
        self.architecture = 'resnet18'
        print(f"‚úÖ –°–æ–∑–¥–∞–Ω–∞ –≤—Å—Ç—Ä–æ–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å: {self.architecture}")
    
    def preprocess_for_classifier(self, x):
        """
        –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏–∑ –¥–∏—Ñ—Ñ—É–∑–∏–∏ –≤ —Ñ–æ—Ä–º–∞—Ç –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞
        
        –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏–∑ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ (128x128, [-1,1])
        –≤ —Ñ–æ—Ä–º–∞—Ç –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞ (224x224, ImageNet –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è)
        """
        
        # –£–±–µ–∂–¥–∞–µ–º—Å—è —á—Ç–æ —Ç–µ–Ω–∑–æ—Ä –Ω–∞ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–µ
        if x.device != next(self.parameters()).device:
            x = x.to(next(self.parameters()).device)
        
        # –ò–∑ –¥–∏–∞–ø–∞–∑–æ–Ω–∞ [-1, 1] –≤ [0, 1]
        x = torch.clamp((x + 1.0) / 2.0, 0, 1)
        
        # –ò–∑–º–µ–Ω–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞ —Å 128x128 –Ω–∞ 224x224
        if x.shape[-1] != CLASSIFIER_IMAGE_SIZE or x.shape[-2] != CLASSIFIER_IMAGE_SIZE:
            x = F.interpolate(
                x, 
                size=(CLASSIFIER_IMAGE_SIZE, CLASSIFIER_IMAGE_SIZE),
                mode='bilinear',
                align_corners=False,
                antialias=True  # –°–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–π –ø–∞—Ä–∞–º–µ—Ç—Ä –¥–ª—è –ª—É—á—à–µ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞
            )
        
        # ImageNet –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è (—Å—Ç–∞–Ω–¥–∞—Ä—Ç –¥–ª—è –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–∞ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π)
        normalize = transforms.Normalize(
            mean=[0.485, 0.456, 0.406],
            std=[0.229, 0.224, 0.225]
        )
        
        x = normalize(x)
        return x
    
    def forward(self, x):
        """Forward pass —Å –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–æ–π"""
        x = self.preprocess_for_classifier(x)
        return self.model(x)
    
    def get_probabilities(self, x):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Å–∞"""
        logits = self.forward(x)
        return F.softmax(logits, dim=1)
    
    def get_per_class_score(self, x, target_class):
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ per-class score –¥–ª—è XAI –∞–Ω–∞–ª–∏–∑–∞
        
        –§–æ—Ä–º—É–ª–∞: y = log p_cl(c | x_t)
        –≥–¥–µ c - —Ü–µ–ª–µ–≤–æ–π –∫–ª–∞—Å—Å, x_t - –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –Ω–∞ –≤—Ä–µ–º–µ–Ω–Ω–æ–º —à–∞–≥–µ t
        
        Args:
            x: –≤—Ö–æ–¥–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            target_class: –∏–Ω–¥–µ–∫—Å —Ü–µ–ª–µ–≤–æ–≥–æ –∫–ª–∞—Å—Å–∞
        
        Returns:
            torch.Tensor: –ª–æ–≥–∞—Ä–∏—Ñ–º–∏—á–µ—Å–∫–∞—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –¥–ª—è –∫–ª–∞—Å—Å–∞
        """
        probs = self.get_probabilities(x)
        # –î–æ–±–∞–≤–ª—è–µ–º –Ω–µ–±–æ–ª—å—à–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –¥–ª—è —á–∏—Å–ª–µ–Ω–Ω–æ–π —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
        return torch.log(probs[:, target_class] + 1e-8)
    
    def predict(self, x):
        """–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –∫–ª–∞—Å—Å–∞"""
        with torch.no_grad():
            logits = self.forward(x)
            return torch.argmax(logits, dim=1)
    
    def get_confidence(self, x, target_class):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏ –≤ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–∏"""
        with torch.no_grad():
            probs = self.get_probabilities(x)
            return probs[:, target_class]


print("‚úÖ –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –º–æ–¥–µ–ª–µ–π –æ–ø—Ä–µ–¥–µ–ª–µ–Ω—ã!")
print(f"üèóÔ∏è  UNet2DModel: {DDPM_IMAGE_SIZE}x{DDPM_IMAGE_SIZE}, {DDPM_CHANNELS} –∫–∞–Ω–∞–ª–æ–≤")
print(f"üß† –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä: –∞–¥–∞–ø—Ç–∏–≤–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞")
print(f"üìè Scheduler: {DDPM_BETA_SCHEDULE}, {DDPM_TIMESTEPS} —à–∞–≥–æ–≤")

def load_classifier_with_fallback():
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º fallback
    
    –ü—ã—Ç–∞–µ—Ç—Å—è –∑–∞–≥—Ä—É–∑–∏—Ç—å –≤–µ—Å–∞ –∏–∑ checkpoint, –ø—Ä–∏ –Ω–µ—É–¥–∞—á–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å
    """
    
    print("üè• –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞...")
    
    # –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å
    classifier = MelanomaClassifierAdaptive(
        num_classes=NUM_CLASSES+1,
        architecture='resnet18',
        pretrained=True
    ).to(device)
    
    # –ü–æ–ø—ã—Ç–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –≤–µ—Å–æ–≤
    if CLASSIFIER_PATH.exists():
        try:
            print(f"üì• –ó–∞–≥—Ä—É–∑–∫–∞ –≤–µ—Å–æ–≤ –∏–∑ {CLASSIFIER_PATH.name}...")
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º checkpoint
            checkpoint = torch.load(CLASSIFIER_PATH, map_location=device)
            
            # –ü–æ–ª—É—á–∞–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ –º–æ–¥–µ–ª–∏
            model_state = classifier.state_dict()
            
            # –§–∏–ª—å—Ç—Ä—É–µ–º —Å–æ–≤–º–µ—Å—Ç–∏–º—ã–µ –∫–ª—é—á–∏
            if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:
                # Checkpoint —Å–æ–¥–µ—Ä–∂–∏—Ç –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
                state_dict = checkpoint['model_state_dict']
            else:
                # Checkpoint —Å–æ–¥–µ—Ä–∂–∏—Ç —Ç–æ–ª—å–∫–æ –≤–µ—Å–∞
                state_dict = checkpoint
            #print(set(state_dict.keys()) - set(model_state.keys()))
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å –∫–ª—é—á–µ–π
            compatible_keys = {}
            incompatible_keys = []
            
            for key, value in state_dict.items():
                if key in model_state and model_state[key].shape == value.shape:
                    compatible_keys[key] = value
                else:
                    incompatible_keys.append(key)
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å–æ–≤–º–µ—Å—Ç–∏–º—ã–µ –≤–µ—Å–∞
            if compatible_keys:
                classifier.load_state_dict(compatible_keys, strict=False)
                print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(compatible_keys)}/{len(state_dict)} –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤")
                
                if incompatible_keys:
                    print(f"‚ö†Ô∏è  –ù–µ—Å–æ–≤–º–µ—Å—Ç–∏–º—ã–µ –∫–ª—é—á–∏ ({len(incompatible_keys)}): {incompatible_keys[:5]}{'...' if len(incompatible_keys) > 5 else ''}")
            else:
                print("‚ùå –ù–µ—Ç —Å–æ–≤–º–µ—Å—Ç–∏–º—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å")
                
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ checkpoint: {e}")
            print("üîÑ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å")
    else:
        print(f"‚ö†Ô∏è  Checkpoint –Ω–µ –Ω–∞–π–¥–µ–Ω: {CLASSIFIER_PATH}")
        print("üîÑ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å")
    
    classifier.eval()
    
    # –¢–µ—Å—Ç —Ä–∞–±–æ—Ç–æ—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏
    try:
        with torch.no_grad():
            test_input = torch.randn(1, DDPM_CHANNELS, DDPM_IMAGE_SIZE, DDPM_IMAGE_SIZE).to(device)
            test_output = classifier(test_input)
            test_probs = classifier.get_probabilities(test_input)
            
            print(f"üß™ –¢–µ—Å—Ç: –≤—Ö–æ–¥ {tuple(test_input.shape)} ‚Üí –≤—ã—Ö–æ–¥ {tuple(test_output.shape)}")
            print(f"üìä –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏: {tuple(test_probs.shape)}, —Å—É–º–º–∞: {test_probs.sum():.3f}")
            
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ —Ç–µ—Å—Ç–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞: {e}")
        return None
    
    print(f"‚úÖ –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –≥–æ—Ç–æ–≤! –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞: {classifier.architecture}")
    return classifier


def load_ddpm_model_for_class(class_name, verbose=True):
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç DDPM –º–æ–¥–µ–ª—å –¥–ª—è —É–∫–∞–∑–∞–Ω–Ω–æ–≥–æ –∫–ª–∞—Å—Å–∞
    
    Args:
        class_name: –Ω–∞–∑–≤–∞–Ω–∏–µ –∫–ª–∞—Å—Å–∞
        verbose: –≤—ã–≤–æ–¥–∏—Ç—å –ø–æ–¥—Ä–æ–±–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
    
    Returns:
        tuple: (unet_model, scheduler) –∏–ª–∏ (None, None) –ø—Ä–∏ –æ—à–∏–±–∫–µ
    """
    
    if class_name not in DDPM_MODELS:
        available_classes = list(DDPM_MODELS.keys())
        print(f"‚ùå –ö–ª–∞—Å—Å '{class_name}' –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω. –î–æ—Å—Ç—É–ø–Ω—ã–µ: {available_classes}")
        return None, None
    
    if verbose:
        print(f"üß¨ –ó–∞–≥—Ä—É–∑–∫–∞ DDPM –º–æ–¥–µ–ª–∏ –¥–ª—è –∫–ª–∞—Å—Å–∞ '{class_name}'...")
    
    try:
        # –°–æ–∑–¥–∞–µ–º –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É
        unet_model = create_ddpm_unet().to(device)
        scheduler = create_ddpm_scheduler()
        
        # –ü—É—Ç—å –∫ –º–æ–¥–µ–ª–∏
        # –†–∞–∑—Ä–µ—à–∞–µ–º –ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–æ—á–Ω–æ–≥–æ –ø—É—Ç–∏ –∫ –º–æ–¥–µ–ª–∏ –∏–∑ –æ–∫—Ä—É–∂–µ–Ω–∏—è (–¥–ª—è –ø–æ–±–∏—Ç–æ–≤–æ–≥–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è)
        override_path = os.environ.get('XAI_DDPM_MODEL_PATH', '').strip()
        if override_path:
            model_path = Path(override_path)
            model_file = model_path.name  # –ò–º—è —Ñ–∞–π–ª–∞ –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
        else:
            model_file = DDPM_MODELS[class_name]
            model_path = CHECKPOINTS_DIR / model_file
        
        if not model_path.exists():
            print(f"‚ùå –§–∞–π–ª –º–æ–¥–µ–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω: {model_path}")
            return None, None
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤–µ—Å–∞
        if verbose:
            print(f"üì• –ó–∞–≥—Ä—É–∑–∫–∞ –≤–µ—Å–æ–≤ –∏–∑ {model_file}...")
        
        state_dict = torch.load(model_path, map_location=device)
        unet_model.load_state_dict(state_dict, strict=True)
        unet_model.eval()
        
        # –¢–µ—Å—Ç –º–æ–¥–µ–ª–∏
        with torch.no_grad():
            test_input = torch.randn(1, DDPM_CHANNELS, DDPM_IMAGE_SIZE, DDPM_IMAGE_SIZE).to(device)
            test_timestep = torch.randint(0, 1000, (1,)).to(device)
            test_output = unet_model(test_input, test_timestep).sample
            
            if verbose:
                print(f"üß™ –¢–µ—Å—Ç: {tuple(test_input.shape)} + t ‚Üí {tuple(test_output.shape)}")
        
        if verbose:
            print(f"‚úÖ DDPM –º–æ–¥–µ–ª—å '{class_name}' –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ!")
        
        return unet_model, scheduler
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ DDPM –º–æ–¥–µ–ª–∏ '{class_name}': {e}")
        return None, None


# === –ò–ù–ò–¶–ò–ê–õ–ò–ó–ê–¶–ò–Ø –ú–û–î–ï–õ–ï–ô ===

print("üöÄ === –ó–ê–ì–†–£–ó–ö–ê –ú–û–î–ï–õ–ï–ô ===")

# –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä
classifier = load_classifier_with_fallback()

if classifier is None:
    print("‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: –Ω–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä")
    raise RuntimeError("–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –Ω–µ –º–æ–∂–µ—Ç –±—ã—Ç—å –∑–∞–≥—Ä—É–∂–µ–Ω")

# –í—ã–±–æ—Ä –∫–ª–∞—Å—Å–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ (–º–æ–∂–Ω–æ –ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å —á–µ—Ä–µ–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –æ–∫—Ä—É–∂–µ–Ω–∏—è XAI_TARGET_CLASS)
TARGET_CLASS_NAME = os.environ.get('XAI_TARGET_CLASS', 'MEL')  # –ò–ó–ú–ï–ù–ò–¢–ï –ù–ê –ù–£–ñ–ù–´–ô –ö–õ–ê–°–°

if TARGET_CLASS_NAME not in CLASS_NAMES:
    print(f"‚ùå –ù–µ–≤–µ—Ä–Ω—ã–π –∫–ª–∞—Å—Å '{TARGET_CLASS_NAME}'. –î–æ—Å—Ç—É–ø–Ω—ã–µ: {CLASS_NAMES}")
    TARGET_CLASS_NAME = CLASS_NAMES[0]  # –í—ã–±–∏—Ä–∞–µ–º –ø–µ—Ä–≤—ã–π –¥–æ—Å—Ç—É–ø–Ω—ã–π
    print(f"üîÑ –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –≤—ã–±—Ä–∞–Ω –∫–ª–∞—Å—Å: {TARGET_CLASS_NAME}")

TARGET_CLASS_ID = CLASS_NAMES.index(TARGET_CLASS_NAME)

print(f"üéØ –¶–µ–ª–µ–≤–æ–π –∫–ª–∞—Å—Å: {TARGET_CLASS_NAME} (–∏–Ω–¥–µ–∫—Å: {TARGET_CLASS_ID})")

# –ó–∞–≥—Ä—É–∂–∞–µ–º DDMP –º–æ–¥–µ–ª—å –¥–ª—è –≤—ã–±—Ä–∞–Ω–Ω–æ–≥–æ –∫–ª–∞—Å—Å–∞
unet_model, scheduler = load_ddpm_model_for_class(TARGET_CLASS_NAME)

if unet_model is None:
    print(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å DDPM –º–æ–¥–µ–ª—å –¥–ª—è –∫–ª–∞—Å—Å–∞ '{TARGET_CLASS_NAME}'")
    print("   –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –Ω–∞–ª–∏—á–∏–µ —Ñ–∞–π–ª–∞ –º–æ–¥–µ–ª–∏ –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ checkpoints/")
    
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –¥–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏
    available_models = []
    for class_name in CLASS_NAMES:
        if class_name in DDPM_MODELS:
            model_path = CHECKPOINTS_DIR / DDPM_MODELS[class_name]
            if model_path.exists():
                available_models.append(class_name)
    
    if available_models:
        print(f"üí° –î–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏: {', '.join(available_models)}")
        print(f"   –ò–∑–º–µ–Ω–∏—Ç–µ TARGET_CLASS_NAME –Ω–∞ –æ–¥–∏–Ω –∏–∑ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –∫–ª–∞—Å—Å–æ–≤")
    else:
        print("üí° –ù–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã—Ö DDPM –º–æ–¥–µ–ª–µ–π. –£–±–µ–¥–∏—Ç–µ—Å—å —á—Ç–æ —Ñ–∞–π–ª—ã –Ω–∞—Ö–æ–¥—è—Ç—Å—è –≤ checkpoints/")
    
    XAI_READY = False
else:
    XAI_READY = True
    print("üöÄ –í—Å–µ –º–æ–¥–µ–ª–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã —É—Å–ø–µ—à–Ω–æ! –ì–æ—Ç–æ–≤ –∫ XAI –∞–Ω–∞–ª–∏–∑—É.")

# –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–∞–º—è—Ç–∏
if device.type == 'cuda':
    torch.cuda.empty_cache()
    gc.collect()
    print(f"üßπ –ü–∞–º—è—Ç—å GPU –æ—á–∏—â–µ–Ω–∞. –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è: {torch.cuda.memory_allocated(device) / 1024**2:.1f} MB")


def generate_trajectory_optimized(unet_model, scheduler, 
                                num_inference_steps=INFERENCE_STEPS,
                                save_every=SAVE_EVERY_N_STEPS,
                                seed=GENERATION_SEED,
                                use_autocast=True):
    """
    –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω–æ–π —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏
    
    –û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏:
    - –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –ø–∞–º—è—Ç—å—é
    - Mixed precision –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è
    - –ü—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä
    - –ü—Ä–æ–≤–µ—Ä–∫–∏ –Ω–∞ –æ—à–∏–±–∫–∏
    
    Args:
        unet_model: –æ–±—É—á–µ–Ω–Ω–∞—è UNet –º–æ–¥–µ–ª—å
        scheduler: DDPM scheduler
        num_inference_steps: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —à–∞–≥–æ–≤ –¥–µ–Ω–æ–π–∑–∏–Ω–≥–∞
        save_every: —Å–æ—Ö—Ä–∞–Ω—è—Ç—å –∫–∞–∂–¥—ã–π N-—ã–π —à–∞–≥
        seed: seed –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏
        use_autocast: –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å mixed precision
    
    Returns:
        tuple: (trajectory, timesteps, metadata)
    """
    
    print(f"üé¨ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω–æ–π —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏ –¥–ª—è –∫–ª–∞—Å—Å–∞ '{TARGET_CLASS_NAME}'...")
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏ –∏ —Å–æ–∑–¥–∞–Ω–∏–µ –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ª–æ–∫–∞–ª—å–Ω—ã–π torch.Generator, —á—Ç–æ–±—ã —Å–æ—Å—Ç–æ—è–Ω–∏–µ –≥–ª–æ–±–∞–ª—å–Ω–æ–≥–æ RNG –Ω–µ –≤–ª–∏—è–ª–æ –Ω–∞ –Ω–∞—á–∞–ª—å–Ω—ã–π —à—É–º
    try:
        torch_gen = torch.Generator(device=device)
        torch_gen.manual_seed(int(seed))
    except Exception:
        # –§–æ–ª–±—ç–∫: —É—Å—Ç–∞–Ω–æ–≤–∏–º –≥–ª–æ–±–∞–ª—å–Ω—ã–µ —Å–∏–¥—ã
        torch.manual_seed(int(seed))
        if torch.cuda.is_available():
            torch.cuda.manual_seed(int(seed))
            torch.cuda.manual_seed_all(int(seed))
        torch_gen = None
    np.random.seed(int(seed))
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –Ω–∞—á–∞–ª—å–Ω–æ–≥–æ —à—É–º–∞ (—Å–æ–≤–º–µ—Å—Ç–∏–º–æ —Å –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π –≤ core)
    shape = (1, DDPM_CHANNELS, DDPM_IMAGE_SIZE, DDPM_IMAGE_SIZE)
    if torch_gen is not None:
        initial_noise = torch.randn(shape, device=device, dtype=torch.float32, generator=torch_gen)
    else:
        initial_noise = torch.randn(shape, device=device, dtype=torch.float32)
    
    print(f"üî¢ –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏:")
    print(f"   –®–∞–≥–∏: {num_inference_steps}, —Å–æ—Ö—Ä–∞–Ω—è—Ç—å –∫–∞–∂–¥—ã–π: {save_every}")
    print(f"   –†–∞–∑–º–µ—Ä: {shape}, —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}")
    print(f"   Mixed precision: {use_autocast and device.type == 'cuda'}")
    print(f"   Seed: {seed}")
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ scheduler
    scheduler.set_timesteps(num_inference_steps, device=device)
    timesteps = scheduler.timesteps

    # –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞: –ø–µ—á–∞—Ç–∞–µ–º –ø–µ—Ä–≤—ã–π –∏ –ø–æ—Å–ª–µ–¥–Ω–∏–π t, —á—Ç–æ–±—ã —É–±–µ–¥–∏—Ç—å—Å—è –≤ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–∏
    try:
        t0 = float(timesteps[0])
        t_last = float(timesteps[-1])
        print(f"üß≠ Timesteps dir: first={t0:.0f} last={t_last:.0f} (–æ–∂–∏–¥–∞–µ—Ç—Å—è start‚âà999 ‚Üí last=0)")
    except Exception:
        pass
    
    # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —à–∞–≥–æ–≤ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
    # –†–µ–∂–∏–º 1: –æ–±—ã—á–Ω—ã–π (–∫–∞–∂–¥—ã–µ save_every –ø–æ –∏–Ω–¥–µ–∫—Å—É —à–∞–≥–∞)
    save_indices = set(range(0, num_inference_steps, save_every))
    if (num_inference_steps - 1) not in save_indices:
        save_indices.add(num_inference_steps - 1)  # –í—Å–µ–≥–¥–∞ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π

    # –†–µ–∂–∏–º 2: –∞–±—Å–æ–ª—é—Ç–Ω—ã–µ t (–Ω–∞–ø—Ä–∏–º–µ—Ä, save_every=250 –ø—Ä–∏ 50 —à–∞–≥–∞—Ö)
    # –ü–æ–¥–±–∏—Ä–∞–µ–º –±–ª–∏–∂–∞–π—à–∏–µ –¥–æ—Å—Ç—É–ø–Ω—ã–µ –∏–Ω–¥–µ–∫—Å—ã –ø–æ–¥ —Ü–µ–ª–µ–≤—ã–µ t –∫—Ä–∞—Ç–Ω—ã–µ save_every
    try:
        save_by_absolute_t = save_every >= num_inference_steps
    except Exception:
        save_by_absolute_t = False
    if save_by_absolute_t:
        try:
            t_list = [int(float(t)) for t in timesteps]
            desired_t = set()
            # –í–∫–ª—é—á–∞–µ–º 0 –∏ max (–æ–±—ã—á–Ω–æ ~1000), –∏ –∫—Ä–∞—Ç–Ω—ã–µ save_every
            desired_t.add(0)
            desired_t.add(max(t_list))
            step_val = max(1, int(save_every))
            k = 0
            while k <= 1000:
                desired_t.add(k)
                k += step_val
            # –ù–∞—Ö–æ–¥–∏–º –±–ª–∏–∂–∞–π—à–∏–µ –∏–Ω–¥–µ–∫—Å—ã –∫ —Ü–µ–ª–µ–≤—ã–º t
            for dt in desired_t:
                closest_idx = min(range(len(t_list)), key=lambda i: abs(t_list[i] - dt))
                save_indices.add(closest_idx)
        except Exception:
            pass
    
    trajectory = []
    saved_timesteps = []
    current_image = initial_noise.clone()
    
    # –û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª –¥–µ–Ω–æ–π–∑–∏–Ω–≥–∞
    try:
        unet_model.eval()
        
        with torch.no_grad():
            total_steps = len(timesteps)
            progress_bar = tqdm(
                enumerate(timesteps), 
                total=total_steps,
                desc=f"Denoising {TARGET_CLASS_NAME}",
                ncols=100
            )

            for step_idx, timestep in progress_bar:
                # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
                timestep_tensor = timestep.unsqueeze(0).to(device)
                
                # Forward pass —Å –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–º autocast
                if use_autocast and device.type == 'cuda':
                    with autocast(device_type='cuda'):
                        noise_pred = unet_model(current_image, timestep_tensor).sample
                else:
                    noise_pred = unet_model(current_image, timestep_tensor).sample
                
                # –®–∞–≥ –¥–µ–Ω–æ–π–∑–∏–Ω–≥–∞
                scheduler_output = scheduler.step(noise_pred, timestep, current_image)
                current_image = scheduler_output.prev_sample
                
                # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω–æ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
                save_frame = (step_idx in save_indices)
                if not save_frame and save_by_absolute_t:
                    try:
                        t_int = int(float(timestep))
                        # –°–æ—Ö—Ä–∞–Ω—è–µ–º, –µ—Å–ª–∏ t –∫—Ä–∞—Ç–µ–Ω save_every, –∞ —Ç–∞–∫–∂–µ –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ–º t==0
                        if (t_int % max(1, save_every) == 0) or (t_int == 0):
                            save_frame = True
                    except Exception:
                        save_frame = False
                if save_frame:
                    # –ö–æ–ø–∏—Ä—É–µ–º –Ω–∞ CPU –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ GPU –ø–∞–º—è—Ç–∏
                    trajectory.append(current_image.detach().cpu().clone())
                    saved_timesteps.append(float(timestep))
                
                # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–∞
                if step_idx % 10 == 0:
                    progress_bar.set_postfix({
                        't': f'{float(timestep):.0f}',
                        'saved': len(trajectory),
                        'mem': f'{torch.cuda.memory_allocated(device) / 1024**2:.0f}MB' if device.type == 'cuda' else 'N/A'
                    })
                    # –õ–æ–≥–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç–æ–≤—ã–π –ø—Ä–æ–≥—Ä–µ—Å—Å –¥–ª—è GUI –ª–æ–≥–æ–≤
                    try:
                        _log_progress_bar("Denoising", step_idx + 1, total_steps)
                    except Exception:
                        pass
                
                # –û—á–∏—Å—Ç–∫–∞ –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã—Ö —Ç–µ–Ω–∑–æ—Ä–æ–≤
                del noise_pred
                if step_idx % 5 == 0 and device.type == 'cuda':
                    torch.cuda.empty_cache()
            
            progress_bar.close()
    
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –≤–æ –≤—Ä–µ–º—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {e}")
        return None, None, None
    
    # –î–æ–±–∞–≤–ª—è–µ–º —Ñ–∏–Ω–∞–ª—å–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –µ—Å–ª–∏ –µ–≥–æ –Ω–µ—Ç
    if len(saved_timesteps) == 0 or saved_timesteps[-1] != 0.0:
        trajectory.append(current_image.detach().cpu().clone())
        saved_timesteps.append(0.0)
    
    # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
    metadata = {
        'class_name': TARGET_CLASS_NAME,
        'class_id': TARGET_CLASS_ID,
        'num_inference_steps': num_inference_steps,
        'save_every': save_every,
        'total_saved': len(trajectory),
        'seed': seed,
        'image_size': DDPM_IMAGE_SIZE,
        'device': str(device),
        'scheduler_type': scheduler.__class__.__name__,
        'beta_schedule': DDPM_BETA_SCHEDULE,
        'generation_time': datetime.now().isoformat()
    }
    
    print(f"‚úÖ –¢—Ä–∞–µ–∫—Ç–æ—Ä–∏—è —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–∞: {len(trajectory)} –∫–∞–¥—Ä–æ–≤")
    print(f"üìä –í—Ä–µ–º–µ–Ω–Ω—ã–µ —à–∞–≥–∏: {[f'{t:.0f}' for t in saved_timesteps]}")
    
    # –§–∏–Ω–∞–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏
    if device.type == 'cuda':
        torch.cuda.empty_cache()
        gc.collect()
    
    return trajectory, saved_timesteps, metadata


def visualize_trajectory(trajectory, timesteps, max_frames=6, figsize=(18, 4)):
    """
    –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω–æ–π —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏
    
    Args:
        trajectory: —Å–ø–∏—Å–æ–∫ —Ç–µ–Ω–∑–æ—Ä–æ–≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
        timesteps: —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —à–∞–≥–∏
        max_frames: –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–∞–¥—Ä–æ–≤ –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è
        figsize: —Ä–∞–∑–º–µ—Ä —Ñ–∏–≥—É—Ä—ã
    """
    
    num_frames = min(max_frames, len(trajectory))
    indices = np.linspace(0, len(trajectory) - 1, num_frames, dtype=int)
    
    fig, axes = plt.subplots(1, num_frames, figsize=figsize)
    if num_frames == 1:
        axes = [axes]
    
    for idx, frame_idx in enumerate(indices):
        # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è —Ç–µ–Ω–∑–æ—Ä–∞ –≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
        img_tensor = trajectory[frame_idx].squeeze()
        
        if img_tensor.dim() == 3 and img_tensor.shape[0] == 3:
            # CHW -> HWC
            img_np = img_tensor.permute(1, 2, 0).numpy()
        else:
            img_np = img_tensor.numpy()
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∏–∑ [-1, 1] –≤ [0, 1]
        img_np = np.clip((img_np + 1.0) / 2.0, 0, 1)
        
        # –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ
        axes[idx].imshow(img_np)
        axes[idx].set_title(f't = {timesteps[frame_idx]:.0f}', fontsize=11, pad=10)
        axes[idx].axis('off')
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Ä–∞–º–∫—É –¥–ª—è —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        if frame_idx == len(trajectory) - 1:
            axes[idx].add_patch(plt.Rectangle((0, 0), img_np.shape[1]-1, img_np.shape[0]-1, 
                                           fill=False, edgecolor='red', linewidth=3))
    
    plt.suptitle(f'üß¨ –î–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω–∞—è —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏—è: {TARGET_CLASS_NAME}', 
                fontsize=14, y=1.05, weight='bold')
    plt.tight_layout()
    plt.show()
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
    final_image = trajectory[-1].squeeze()
    print(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è:")
    print(f"   –†–∞–∑–º–µ—Ä: {tuple(final_image.shape)}")
    print(f"   –î–∏–∞–ø–∞–∑–æ–Ω: [{final_image.min():.3f}, {final_image.max():.3f}]")
    print(f"   –°—Ä–µ–¥–Ω–µ–µ: {final_image.mean():.3f}, std: {final_image.std():.3f}")


# === –ì–ï–ù–ï–†–ê–¶–ò–Ø –¢–†–ê–ï–ö–¢–û–†–ò–ò ===

if XAI_READY:
    print("üé¨ === –ì–ï–ù–ï–†–ê–¶–ò–Ø –î–ò–§–§–£–ó–ò–û–ù–ù–û–ô –¢–†–ê–ï–ö–¢–û–†–ò–ò ===")
    
    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏—é
    trajectory, timesteps, gen_metadata = generate_trajectory_optimized(
        unet_model=unet_model,
        scheduler=scheduler,
        num_inference_steps=INFERENCE_STEPS,
        save_every=SAVE_EVERY_N_STEPS,
        seed=GENERATION_SEED,
        use_autocast=False
    )
    
    if trajectory is not None:
        # –í–∏–∑—É–∞–ª–∏–∑–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        visualize_trajectory(trajectory, timesteps, max_frames=6)
        
        print(f"üìã –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏:")
        for key, value in gen_metadata.items():
            if key != 'generation_time':
                print(f"   {key}: {value}")
        
        TRAJECTORY_READY = True
    else:
        print("‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏")
        TRAJECTORY_READY = False
else:
    print("‚ö†Ô∏è  –ü—Ä–æ–ø—É—Å–∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: –º–æ–¥–µ–ª–∏ –Ω–µ –≥–æ—Ç–æ–≤—ã")
    TRAJECTORY_READY = False


class ModernXAIAnalyzer:
    """
    –°–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–π XAI –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –¥–ª—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
    
    –†–µ–∞–ª–∏–∑—É–µ—Ç —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º–µ—Ç–æ–¥—ã –æ–±—ä—è—Å–Ω–∏–º–æ–≥–æ –ò–ò:
    - Integrated Gradients —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è–º–∏
    - SHAP –∞–ø–ø—Ä–æ–∫—Å–∏–º–∞—Ü–∏—è —á–µ—Ä–µ–∑ –ø–∞—Ç—á–∏
    - Time-SHAP –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω–æ–π –≤–∞–∂–Ω–æ—Å—Ç–∏
    - –ì—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–µ –º–µ—Ç–æ–¥—ã –∫–∞–∫ fallback
    """
    
    def __init__(self, classifier, device, verbose=True):
        self.classifier = classifier
        self.device = device
        self.verbose = verbose
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è XAI –º–µ—Ç–æ–¥–æ–≤
        self.ig_method = None
        self.gradient_shap = None
        
        if CAPTUM_AVAILABLE:
            try:
                self.ig_method = IntegratedGradients(self._model_wrapper)
                self.gradient_shap = GradientShap(self._model_wrapper)
                if verbose:
                    print("‚úÖ Captum –º–µ—Ç–æ–¥—ã –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω—ã")
            except Exception as e:
                if verbose:
                    print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ Captum: {e}")
        
        # –ö—ç—à –¥–ª—è –±–∞–∑–æ–≤—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
        self._baseline_cache = {}
        
        if verbose:
            print(f"üî¨ XAI Analyzer –≥–æ—Ç–æ–≤. Captum: {CAPTUM_AVAILABLE}")
    
    def _model_wrapper(self, x):
        """–û–±—ë—Ä—Ç–∫–∞ –º–æ–¥–µ–ª–∏ –¥–ª—è Captum"""
        return self.classifier(x)
    
    def _get_baseline(self, image, baseline_type='noise'):
        """
        –ü–æ–ª—É—á–∞–µ—Ç baseline –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –¥–ª—è XAI –º–µ—Ç–æ–¥–æ–≤
        
        Args:
            image: –≤—Ö–æ–¥–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            baseline_type: —Ç–∏–ø baseline ('noise', 'zero', 'blur')
        
        Returns:
            torch.Tensor: baseline –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
        """
        
        cache_key = f"{baseline_type}_{image.shape}_{image.device}"
        
        if cache_key not in self._baseline_cache:
            if baseline_type == 'noise':
                baseline = torch.randn_like(image) * 0.1
            elif baseline_type == 'zero':
                baseline = torch.zeros_like(image)
            elif baseline_type == 'blur':
                # –°–∏–ª—å–Ω–æ–µ —Ä–∞–∑–º—ã—Ç–∏–µ –∫–∞–∫ baseline
                baseline = F.avg_pool2d(image, kernel_size=31, stride=1, padding=15)
            else:
                baseline = torch.zeros_like(image)
            
            self._baseline_cache[cache_key] = baseline
        
        return self._baseline_cache[cache_key]
    
    def compute_integrated_gradients(self, image, target_class, n_steps=IG_N_STEPS, baseline_type='noise'):
        """
        –í—ã—á–∏—Å–ª—è–µ—Ç Integrated Gradients
        
        –§–æ—Ä–º—É–ª–∞: IG_i(x) = (x_i - x'_i) √ó ‚à´[0,1] ‚àÇF(x' + Œ±(x - x'))/‚àÇx_i dŒ±
        
        Args:
            image: –≤—Ö–æ–¥–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            target_class: –∏–Ω–¥–µ–∫—Å —Ü–µ–ª–µ–≤–æ–≥–æ –∫–ª–∞—Å—Å–∞
            n_steps: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —à–∞–≥–æ–≤ –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–∏—è
            baseline_type: —Ç–∏–ø baseline –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        
        Returns:
            torch.Tensor: –∫–∞—Ä—Ç–∞ –∞—Ç—Ä–∏–±—É—Ü–∏–∏
        """
        
        image = image.to(self.device)
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º Captum –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω
        if self.ig_method is not None:
            try:
                baseline = self._get_baseline(image, baseline_type)
                
                # –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è per-class score
                def target_func(x):
                    return self.classifier.get_per_class_score(x, target_class)
                
                # –í—Ä–µ–º–µ–Ω–Ω–∞—è –∑–∞–º–µ–Ω–∞
                original_func = self.ig_method.forward_func
                self.ig_method.forward_func = target_func
                
                try:
                    attribution = self.ig_method.attribute(
                        image,
                        baselines=baseline,
                        n_steps=n_steps,
                        method='riemann_right'  # –ë–æ–ª–µ–µ —Å—Ç–∞–±–∏–ª—å–Ω—ã–π –º–µ—Ç–æ–¥
                    )
                finally:
                    # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Ñ—É–Ω–∫—Ü–∏—é
                    self.ig_method.forward_func = original_func
                
                return attribution
                
            except Exception as e:
                if self.verbose:
                    print(f"‚ö†Ô∏è  Captum IG failed: {e}. Using gradient approximation.")
        
        # Fallback: –ø—Ä–æ—Å—Ç–æ–π –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π –º–µ—Ç–æ–¥
        return self._compute_gradient_attribution(image, target_class)
    
    def _compute_gradient_attribution(self, image, target_class):
        """–ì—Ä–∞–¥–∏–µ–Ω—Ç–Ω–∞—è –∞—Ç—Ç—Ä–∏–±—É—Ü–∏—è –∫–∞–∫ fallback"""
        
        image = image.to(self.device)
        image.requires_grad_(True)
        
        # Forward pass
        score = self.classifier.get_per_class_score(image, target_class)
        
        # Backward pass
        score.backward()
        
        # –ü–æ–ª—É—á–∞–µ–º –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã
        attribution = image.grad.clone()
        
        # –û—á–∏—Å—Ç–∫–∞
        image.grad.zero_()
        image.requires_grad_(False)
        
        return attribution
    
    def compute_shap_approximation(self, image, target_class, 
                                 n_samples=SHAP_N_SAMPLES, patch_size=16):
        """
        SHAP –∞–ø–ø—Ä–æ–∫—Å–∏–º–∞—Ü–∏—è —á–µ—Ä–µ–∑ –ø–∞—Ç—á–∏
        
        –†–µ–∞–ª–∏–∑—É–µ—Ç —É–ø—Ä–æ—â—ë–Ω–Ω—É—é –≤–µ—Ä—Å–∏—é Kernel SHAP —Å –ø–∞—Ç—á–∞–º–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        
        Args:
            image: –≤—Ö–æ–¥–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            target_class: –∏–Ω–¥–µ–∫—Å —Ü–µ–ª–µ–≤–æ–≥–æ –∫–ª–∞—Å—Å–∞
            n_samples: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—ç–º–ø–ª–æ–≤ –¥–ª—è –∞–ø–ø—Ä–æ–∫—Å–∏–º–∞—Ü–∏–∏
            patch_size: —Ä–∞–∑–º–µ—Ä –ø–∞—Ç—á–∞
        
        Returns:
            torch.Tensor: SHAP –∫–∞—Ä—Ç–∞ –∞—Ç—Ä–∏–±—É—Ü–∏–∏
        """
        
        image = image.to(self.device)
        batch_size, channels, height, width = image.shape
        
        # –°–æ–∑–¥–∞—ë–º —Å–µ—Ç–∫—É –ø–∞—Ç—á–µ–π
        n_patches_h = height // patch_size
        n_patches_w = width // patch_size
        
        attribution = torch.zeros_like(image)
        
        # Baseline score (—á—ë—Ä–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ)
        baseline_image = torch.zeros_like(image)
        with torch.no_grad():
            baseline_score = self.classifier.get_per_class_score(
                baseline_image, target_class
            ).item()
        
        # –°–ª—É—á–∞–π–Ω–æ–µ —Å—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –º–∞—Å–æ–∫
        for sample_idx in range(n_samples):
            # –°–æ–∑–¥–∞—ë–º —Å–ª—É—á–∞–π–Ω—É—é –º–∞—Å–∫—É –ø–∞—Ç—á–µ–π
            patch_mask = torch.rand(n_patches_h, n_patches_w) > 0.5
            
            # –†–∞—Å—à–∏—Ä—è–µ–º –¥–æ —Ä–∞–∑–º–µ—Ä–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            full_mask = torch.zeros(height, width, dtype=torch.bool)
            
            for i in range(n_patches_h):
                for j in range(n_patches_w):
                    if patch_mask[i, j]:
                        y_start, y_end = i * patch_size, (i + 1) * patch_size
                        x_start, x_end = j * patch_size, (j + 1) * patch_size
                        full_mask[y_start:y_end, x_start:x_end] = True
            
            # –ü—Ä–∏–º–µ–Ω—è–µ–º –º–∞—Å–∫—É –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é
            masked_image = image.clone()
            masked_image[:, :, ~full_mask] = 0
            
            # –ü–æ–ª—É—á–∞–µ–º score
            with torch.no_grad():
                masked_score = self.classifier.get_per_class_score(
                    masked_image, target_class
                ).item()
            
            # –í–∫–ª–∞–¥ –≤–∏–¥–∏–º—ã—Ö –ø–∞—Ç—á–µ–π
            contribution = masked_score - baseline_score
            mask_tensor = full_mask.unsqueeze(0).unsqueeze(0).float().to(self.device)
            attribution += contribution * mask_tensor
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
        attribution /= n_samples
        
        return attribution
    
    def compute_time_shap(self, trajectory, timesteps, target_class):
        """
        –í—ã—á–∏—Å–ª—è–µ—Ç Time-SHAP: –≤–∞–∂–Ω–æ—Å—Ç—å –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —à–∞–≥–æ–≤
        
        –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç, –∫–∞–∫–∏–µ –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —à–∞–≥–∏ t –Ω–∞–∏–±–æ–ª–µ–µ –≤–∞–∂–Ω—ã
        –¥–ª—è —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ —Ä–µ—à–µ–Ω–∏—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞
        
        Args:
            trajectory: —Å–ø–∏—Å–æ–∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –Ω–∞ —Ä–∞–∑–Ω—ã—Ö –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —à–∞–≥–∞—Ö
            timesteps: —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —à–∞–≥–∏
            target_class: –∏–Ω–¥–µ–∫—Å —Ü–µ–ª–µ–≤–æ–≥–æ –∫–ª–∞—Å—Å–∞
        
        Returns:
            tuple: (normalized_importance, raw_scores)
        """
        
        if self.verbose:
            print(f"üïí –í—ã—á–∏—Å–ª–µ–Ω–∏–µ Time-SHAP –¥–ª—è {len(trajectory)} –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —à–∞–≥–æ–≤...")
        
        confidence_scores = []
        prob_scores = []
        
        for i, (image, t) in enumerate(zip(trajectory, timesteps)):
            image = image.to(self.device)
            
            with torch.no_grad():
                # –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞
                confidence = self.classifier.get_confidence(image, target_class).item()
                prob_scores.append(confidence)
                
                # Per-class score –¥–ª—è –±–æ–ª–µ–µ —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
                per_class_score = self.classifier.get_per_class_score(image, target_class).item()
                confidence_scores.append(per_class_score)
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ numpy
        confidence_scores = np.array(confidence_scores)
        prob_scores = np.array(prob_scores)
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –≤–∞–∂–Ω–æ—Å—Ç–∏ (–∏—Å–ø–æ–ª—å–∑—É–µ–º per-class scores)
        if len(confidence_scores) > 1 and (confidence_scores.max() - confidence_scores.min()) > 1e-6:
            normalized_importance = (confidence_scores - confidence_scores.min()) / \
                                  (confidence_scores.max() - confidence_scores.min())
        else:
            normalized_importance = np.ones_like(confidence_scores) / len(confidence_scores)
        
        raw_data = {
            'confidence_scores': confidence_scores,
            'probability_scores': prob_scores,
            'timesteps': timesteps
        }
        
        if self.verbose:
            max_idx = np.argmax(normalized_importance)
            print(f"   –ù–∞–∏–±–æ–ª–µ–µ –≤–∞–∂–Ω—ã–π —à–∞–≥: t={timesteps[max_idx]:.0f} (–≤–∞–∂–Ω–æ—Å—Ç—å: {normalized_importance[max_idx]:.3f})")
        
        return normalized_importance, raw_data
    
    def compute_combined_attribution(self, image, target_class, 
                                   methods=['ig', 'shap'], weights=None):
        """
        –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–∞—è XAI –∞—Ç—Ä–∏–±—É—Ü–∏—è
        
        –û–±—ä–µ–¥–∏–Ω—è–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ –º–µ—Ç–æ–¥–æ–≤ –¥–ª—è –±–æ–ª–µ–µ —Ä–æ–±–∞—Å—Ç–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        
        Args:
            image: –≤—Ö–æ–¥–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            target_class: –∏–Ω–¥–µ–∫—Å —Ü–µ–ª–µ–≤–æ–≥–æ –∫–ª–∞—Å—Å–∞
            methods: —Å–ø–∏—Å–æ–∫ –º–µ—Ç–æ–¥–æ–≤ ['ig', 'shap', 'gradient']
            weights: –≤–µ—Å–∞ –¥–ª—è –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–∏—è (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω—ã–µ)
        
        Returns:
            tuple: (combined_attribution, method_details)
        """
        
        if weights is None:
            weights = [1.0 / len(methods)] * len(methods)
        
        attributions = []
        method_details = {}
        
        for method, weight in zip(methods, weights):
            if self.verbose:
                print(f"   –í—ã—á–∏—Å–ª–µ–Ω–∏–µ {method.upper()}... (–≤–µ—Å: {weight:.2f})")
            
            try:
                if method == 'ig':
                    attr = self.compute_integrated_gradients(image, target_class)
                elif method == 'shap':
                    attr = self.compute_shap_approximation(image, target_class)
                elif method == 'gradient':
                    attr = self._compute_gradient_attribution(image, target_class)
                else:
                    print(f"   ‚ö†Ô∏è  –ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –º–µ—Ç–æ–¥: {method}")
                    continue
                
                attributions.append(attr * weight)
                method_details[method] = {
                    'weight': weight,
                    'mean_attribution': float(torch.mean(torch.abs(attr))),
                    'max_attribution': float(torch.max(torch.abs(attr)))
                }
                
            except Exception as e:
                print(f"   ‚ùå –û—à–∏–±–∫–∞ –≤ –º–µ—Ç–æ–¥–µ {method}: {e}")
                continue
        
        if not attributions:
            raise RuntimeError("–ù–µ —É–¥–∞–ª–æ—Å—å –≤—ã—á–∏—Å–ª–∏—Ç—å –Ω–∏ –æ–¥–Ω—É –∞—Ç—Ä–∏–±—É—Ü–∏—é")
        
        # –ö–æ–º–±–∏–Ω–∏—Ä—É–µ–º –∞—Ç—Ä–∏–±—É—Ü–∏–∏
        combined_attribution = torch.stack(attributions).sum(dim=0)
        
        return combined_attribution, method_details


# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è XAI –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞
if XAI_READY and TRAJECTORY_READY:
    print("üî¨ === –ò–ù–ò–¶–ò–ê–õ–ò–ó–ê–¶–ò–Ø XAI –ê–ù–ê–õ–ò–ó–ê–¢–û–†–ê ===")
    
    xai_analyzer = ModernXAIAnalyzer(
        classifier=classifier,
        device=device,
        verbose=True
    )
    
    # –¢–µ—Å—Ç XAI –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞
    try:
        test_image = trajectory[-1].to(device)  # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ñ–∏–Ω–∞–ª—å–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
        
        print(f"üß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ XAI –º–µ—Ç–æ–¥–æ–≤ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏ {tuple(test_image.shape)}...")
        
        # –¢–µ—Å—Ç IG
        test_ig = xai_analyzer.compute_integrated_gradients(
            test_image, TARGET_CLASS_ID, n_steps=10
        )
        print(f"   ‚úÖ IG: {tuple(test_ig.shape)}, –¥–∏–∞–ø–∞–∑–æ–Ω: [{test_ig.min():.3f}, {test_ig.max():.3f}]")
        
        # –¢–µ—Å—Ç SHAP
        test_shap = xai_analyzer.compute_shap_approximation(
            test_image, TARGET_CLASS_ID, n_samples=5
        )
        print(f"   ‚úÖ SHAP: {tuple(test_shap.shape)}, –¥–∏–∞–ø–∞–∑–æ–Ω: [{test_shap.min():.3f}, {test_shap.max():.3f}]")
        
        # –¢–µ—Å—Ç Time-SHAP
        test_time_importance, _ = xai_analyzer.compute_time_shap(
            trajectory, timesteps, TARGET_CLASS_ID
        )
        print(f"   ‚úÖ Time-SHAP: {len(test_time_importance)} —à–∞–≥–æ–≤")
        
        XAI_ANALYZER_READY = True
        print("üöÄ XAI –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –≥–æ—Ç–æ–≤ –∫ –ø–æ–ª–Ω–æ–º—É –∞–Ω–∞–ª–∏–∑—É!")
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è XAI –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞: {e}")
        XAI_ANALYZER_READY = False
        
else:
    print("‚ö†Ô∏è  XAI –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω: –º–æ–¥–µ–ª–∏ –∏–ª–∏ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏—è –Ω–µ –≥–æ—Ç–æ–≤—ã")
    XAI_ANALYZER_READY = False


def select_regions_advanced(attribution_map, k_percent=TOP_K_PERCENT, 
                           region_type='top', morphology_cleanup=True,
                           connectivity=8):
    """
    –ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è —Ñ—É–Ω–∫—Ü–∏—è –≤—ã–±–æ—Ä–∞ —Ç–æ–ø-k –∏–ª–∏ bottom-k —Ä–µ–≥–∏–æ–Ω–æ–≤
    
    Args:
        attribution_map: –∫–∞—Ä—Ç–∞ –∞—Ç—Ä–∏–±—É—Ü–∏–∏ (tensor –∏–ª–∏ numpy)
        k_percent: –ø—Ä–æ—Ü–µ–Ω—Ç —Ä–µ–≥–∏–æ–Ω–æ–≤ –¥–ª—è –≤—ã–±–æ—Ä–∞
        region_type: 'top' –∏–ª–∏ 'bottom'
        morphology_cleanup: –ø—Ä–∏–º–µ–Ω–∏—Ç—å –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫—É—é –æ—á–∏—Å—Ç–∫—É
        connectivity: —Å–≤—è–∑–Ω–æ—Å—Ç—å –¥–ª—è –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏—Ö –æ–ø–µ—Ä–∞—Ü–∏–π
    
    Returns:
        dict: —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å –º–∞—Å–∫–æ–π, —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ–π –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
    """
    
    # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ numpy
    if torch.is_tensor(attribution_map):
        attr_np = attribution_map.detach().cpu().numpy()
    else:
        attr_np = attribution_map.copy()
    
    # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–µ–π
    original_shape = attr_np.shape
    
    if len(attr_np.shape) == 4:  # Batch dimension
        attr_np = attr_np[0]
    
    if len(attr_np.shape) == 3:  # Channel dimension
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º L2 –Ω–æ—Ä–º—É –ø–æ –∫–∞–Ω–∞–ª–∞–º –¥–ª—è –ª—É—á—à–µ–π —Ä–µ–ø—Ä–µ–∑–µ–Ω—Ç–∞—Ç–∏–≤–Ω–æ—Å—Ç–∏
        attr_np = np.linalg.norm(attr_np, axis=0)
    else:
        attr_np = np.abs(attr_np)
    
    # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –ø–æ—Ä–æ–≥–∞
    flat_attr = attr_np.flatten()
    
    if region_type == 'top':
        threshold = np.percentile(flat_attr, 100 - k_percent)
        mask = attr_np >= threshold
    elif region_type == 'bottom':
        threshold = np.percentile(flat_attr, k_percent)
        mask = attr_np <= threshold
    else:
        raise ValueError(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π region_type: {region_type}")
    
    # –ú–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∞—è –æ—á–∏—Å—Ç–∫–∞
    if morphology_cleanup:
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä—É—é—â–∏–π —ç–ª–µ–º–µ–Ω—Ç
        if connectivity == 4:
            structure = ndimage.generate_binary_structure(2, 1)
        else:  # connectivity == 8
            structure = ndimage.generate_binary_structure(2, 2)
        
        # –ó–∞–∫—Ä—ã—Ç–∏–µ (–∑–∞–ø–æ–ª–Ω–µ–Ω–∏–µ –¥—ã—Ä)
        mask = ndimage.binary_closing(mask, structure=structure, iterations=2)
        
        # –û—Ç–∫—Ä—ã—Ç–∏–µ (—É–¥–∞–ª–µ–Ω–∏–µ –º–µ–ª–∫–∏—Ö –æ–±—ä–µ–∫—Ç–æ–≤)
        mask = ndimage.binary_opening(mask, structure=structure, iterations=1)
        
        # –£–¥–∞–ª–µ–Ω–∏–µ —Å–æ–≤—Å–µ–º –º–µ–ª–∫–∏—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç
        labeled_mask, num_features = ndimage.label(mask, structure=structure)
        if num_features > 0:
            # –û—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –±–æ–ª—å—à–µ –æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞
            component_sizes = ndimage.sum(mask, labeled_mask, range(1, num_features + 1))
            min_size = max(10, int(0.01 * mask.size))  # –ú–∏–Ω–∏–º—É–º 1% –æ—Ç –æ–±—â–µ–≥–æ —Ä–∞–∑–º–µ—Ä–∞
            
            large_components = np.where(component_sizes >= min_size)[0] + 1
            mask = np.isin(labeled_mask, large_components)
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    total_pixels = attr_np.size
    selected_pixels = np.sum(mask)
    actual_percentage = (selected_pixels / total_pixels) * 100
    
    if selected_pixels > 0:
        mean_attribution_selected = np.mean(attr_np[mask])
        std_attribution_selected = np.std(attr_np[mask])
        max_attribution_selected = np.max(attr_np[mask])
        min_attribution_selected = np.min(attr_np[mask])
    else:
        mean_attribution_selected = 0
        std_attribution_selected = 0
        max_attribution_selected = 0
        min_attribution_selected = 0
    
    results = {
        'mask': mask,
        'threshold': threshold,
        'statistics': {
            'total_pixels': total_pixels,
            'selected_pixels': selected_pixels,
            'target_percentage': k_percent,
            'actual_percentage': actual_percentage,
            'threshold_value': threshold,
            'mean_attribution': np.mean(attr_np),
            'std_attribution': np.std(attr_np),
            'mean_attribution_selected': mean_attribution_selected,
            'std_attribution_selected': std_attribution_selected,
            'max_attribution_selected': max_attribution_selected,
            'min_attribution_selected': min_attribution_selected,
        },
        'metadata': {
            'region_type': region_type,
            'morphology_cleanup': morphology_cleanup,
            'connectivity': connectivity,
            'original_shape': original_shape
        }
    }
    
    return results


def counterfactual_intervention_advanced(image, mask, intervention_type='noise',
                                       **kwargs):
    """
    –ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è –∫–æ–Ω—Ç—Ä–∞—Ñ–∞–∫—Ç—É–∞–ª—å–Ω–∞—è –∏–Ω—Ç–µ—Ä–≤–µ–Ω—Ü–∏—è
    
    –†–µ–∞–ª–∏–∑—É–µ—Ç —Ñ–æ—Ä–º—É–ª—É: xÃÉ_t = x_t √ó (1-M) + intervention √ó M
    –≥–¥–µ M - –º–∞—Å–∫–∞ —Ä–µ–≥–∏–æ–Ω–æ–≤ –¥–ª—è –∏–Ω—Ç–µ—Ä–≤–µ–Ω—Ü–∏–∏
    
    Args:
        image: –∏—Å—Ö–æ–¥–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
        mask: –º–∞—Å–∫–∞ —Ä–µ–≥–∏–æ–Ω–æ–≤ (numpy –∏–ª–∏ tensor)
        intervention_type: —Ç–∏–ø –∏–Ω—Ç–µ—Ä–≤–µ–Ω—Ü–∏–∏
        **kwargs: –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
    
    Returns:
        dict: —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏–Ω—Ç–µ—Ä–≤–µ–Ω—Ü–∏–∏
    """
    
    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
    noise_std = kwargs.get('noise_std', NOISE_STD)
    blur_kernel = kwargs.get('blur_kernel', BLUR_KERNEL_SIZE)
    inpaint_method = kwargs.get('inpaint_method', 'telea')
    
    # –£–±–µ–∂–¥–∞–µ–º—Å—è —á—Ç–æ –≤—Å—ë –Ω–∞ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–µ
    device = image.device
    
    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –º–∞—Å–∫–∏
    if isinstance(mask, np.ndarray):
        mask_tensor = torch.from_numpy(mask).float().to(device)
    else:
        mask_tensor = mask.float().to(device)
    
    # –ü—Ä–∏–≤–µ–¥–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–µ–π –º–∞—Å–∫–∏ –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é
    while len(mask_tensor.shape) < len(image.shape):
        mask_tensor = mask_tensor.unsqueeze(0)
    
    # –†–∞—Å—à–∏—Ä–µ–Ω–∏–µ –Ω–∞ –≤—Å–µ –∫–∞–Ω–∞–ª—ã –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
    if len(mask_tensor.shape) == 3 and image.shape[1] == 3:
        mask_tensor = mask_tensor.unsqueeze(1).repeat(1, 3, 1, 1)
    
    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–Ω—Ç–µ—Ä–≤–µ–Ω—Ü–∏–∏ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–∏–ø–∞
    if intervention_type == 'noise':
        intervention = torch.randn_like(image) * noise_std
        
    elif intervention_type == 'gaussian_noise':
        # –ì–∞—É—Å—Å–æ–≤—Å–∫–∏–π —à—É–º —Å –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–º std
        adaptive_std = max(noise_std, image.std().item() * 0.5)
        intervention = torch.randn_like(image) * adaptive_std
        
    elif intervention_type == 'zero':
        intervention = torch.zeros_like(image)
        
    elif intervention_type == 'mean':
        # –ó–∞–º–µ–Ω—è–µ–º –Ω–∞ —Å—Ä–µ–¥–Ω–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        mean_val = image.mean(dim=[-2, -1], keepdim=True)
        intervention = torch.full_like(image, 0) + mean_val
        
    elif intervention_type == 'blur':
        # –ì–∞—É—Å—Å–æ–≤–æ —Ä–∞–∑–º—ã—Ç–∏–µ
        if blur_kernel % 2 == 0:
            blur_kernel += 1
        
        padding = blur_kernel // 2
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º —Ä–∞–∑–º—ã—Ç–∏–µ –ø–æ –∫–∞–Ω–∞–ª–∞–º
        blurred_channels = []
        for c in range(image.shape[1]):
            channel = image[:, c:c+1, :, :]
            blurred = F.avg_pool2d(channel, kernel_size=blur_kernel, 
                                 stride=1, padding=padding)
            blurred_channels.append(blurred)
        
        intervention = torch.cat(blurred_channels, dim=1)
        
    elif intervention_type == 'inpaint':
        # –ü—Ä–æ—Å—Ç–æ–µ –∏–Ω–ø–µ–π–Ω—Ç–∏–Ω–≥ —á–µ—Ä–µ–∑ —Å–≤—ë—Ä—Ç–∫—É
        kernel_size = 5
        padding = kernel_size // 2
        
        # –°–æ–∑–¥–∞—ë–º —è–¥—Ä–æ –¥–ª—è —É—Å—Ä–µ–¥–Ω–µ–Ω–∏—è
        kernel = torch.ones(1, 1, kernel_size, kernel_size).to(device) / (kernel_size ** 2)
        kernel = kernel.repeat(image.shape[1], 1, 1, 1)
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º —Å–≤—ë—Ä—Ç–∫—É –ø–æ –≥—Ä—É–ø–ø–∞–º (–ø–æ –∫–∞–Ω–∞–ª–∞–º)
        intervention = F.conv2d(image, kernel, padding=padding, groups=image.shape[1])
        
    elif intervention_type == 'shuffle':
        # –ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø –≤–µ—Ä—Å–∏—è –ø–µ—Ä–µ–º–µ—à–∏–≤–∞–Ω–∏—è –ø–∏–∫—Å–µ–ª–µ–π
        intervention = image.clone()
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ –ø–µ—Ä–µ–¥ –æ–±—Ä–∞–±–æ—Ç–∫–æ–π
        batch_size, n_channels, height, width = intervention.shape
        
        for b in range(batch_size):
            for c in range(n_channels):
                channel = intervention[b, c]
                
                # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ø—Ä–∞–≤–∏–ª—å–Ω–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ –º–∞—Å–∫–∏
                if len(mask_tensor.shape) == 4 and mask_tensor.shape[1] > c:
                    mask_2d = mask_tensor[b, c]  # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–π –∫–∞–Ω–∞–ª
                elif len(mask_tensor.shape) == 4 and mask_tensor.shape[1] == 1:
                    mask_2d = mask_tensor[b, 0]  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω—ã–π –∫–∞–Ω–∞–ª
                elif len(mask_tensor.shape) == 3:
                    mask_2d = mask_tensor[b]     # –ò—Å–ø–æ–ª—å–∑—É–µ–º –º–∞—Å–∫—É –±–µ–∑ –∫–∞–Ω–∞–ª—å–Ω–æ–≥–æ –∏–∑–º–µ—Ä–µ–Ω–∏—è
                else:
                    mask_2d = mask_tensor[0, 0] if len(mask_tensor.shape) >= 2 else mask_tensor
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –º–∞—Å–∫–∞ –Ω–µ –ø—É—Å—Ç–∞—è
                if mask_2d.sum() > 0:
                    masked_pixels = channel[mask_2d.bool()]
                    if len(masked_pixels) > 1:  # –ù—É–∂–Ω–æ —Ö–æ—Ç—è –±—ã 2 –ø–∏–∫—Å–µ–ª—è –¥–ª—è –ø–µ—Ä–µ–º–µ—à–∏–≤–∞–Ω–∏—è
                        shuffled_pixels = masked_pixels[torch.randperm(len(masked_pixels))]
                        channel[mask_2d.bool()] = shuffled_pixels
    else:
        # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é - —à—É–º
        intervention = torch.randn_like(image) * noise_std
    
    # –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –∏–Ω—Ç–µ—Ä–≤–µ–Ω—Ü–∏–∏ —Å–æ–≥–ª–∞—Å–Ω–æ —Ñ–æ—Ä–º—É–ª–µ
    modified_image = image * (1 - mask_tensor) + intervention * mask_tensor
    
    # –û–±–µ—Å–ø–µ—á–∏–≤–∞–µ–º –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π –¥–∏–∞–ø–∞–∑–æ–Ω –∑–Ω–∞—á–µ–Ω–∏–π
    modified_image = torch.clamp(modified_image, -1, 1)
    
    # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫
    with torch.no_grad():
        # –†–∞–∑–Ω–æ—Å—Ç—å –º–µ–∂–¥—É –æ—Ä–∏–≥–∏–Ω–∞–ª–æ–º –∏ –º–æ–¥–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ–º
        diff = torch.abs(image - modified_image)
        
        results = {
            'modified_image': modified_image,
            'intervention': intervention,
            'mask_tensor': mask_tensor,
            'difference': diff,
            'statistics': {
                'intervention_type': intervention_type,
                'mask_coverage': float(mask_tensor.mean()),
                'mean_difference': float(diff.mean()),
                'max_difference': float(diff.max()),
                'intervention_strength': float(torch.abs(intervention).mean()),
            },
            'parameters': kwargs
        }
    
    return results


def compute_causal_shift_comprehensive(classifier, original_image, modified_image, 
                                     target_class, include_all_classes=True):
    """
    –ö–æ–º–ø–ª–µ–∫—Å–Ω–æ–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏–µ –∫–∞—É–∑–∞–ª—å–Ω–æ–≥–æ —Å–¥–≤–∏–≥–∞ (CFI)
    
    –§–æ—Ä–º—É–ª—ã:
    CFI = g(x_original) - g(x_modified)
    Œ¥ = |CFI| / (|g(x_original)| + Œµ)
    
    Args:
        classifier: –º–æ–¥–µ–ª—å –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞
        original_image: –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
        modified_image: –º–æ–¥–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
        target_class: –∏–Ω–¥–µ–∫—Å —Ü–µ–ª–µ–≤–æ–≥–æ –∫–ª–∞—Å—Å–∞
        include_all_classes: –∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –≤—Å–µ –∫–ª–∞—Å—Å—ã
    
    Returns:
        dict: –ø–æ–¥—Ä–æ–±–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞
    """
    
    with torch.no_grad():
        # Per-class scores
        orig_score = classifier.get_per_class_score(original_image, target_class)
        mod_score = classifier.get_per_class_score(modified_image, target_class)
        
        # –ü–æ–ª–Ω—ã–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏
        orig_probs = classifier.get_probabilities(original_image)
        mod_probs = classifier.get_probabilities(modified_image)
        
        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –∫–ª–∞—Å—Å—ã
        orig_pred = torch.argmax(orig_probs, dim=1)
        mod_pred = torch.argmax(mod_probs, dim=1)
        
        # –û—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        cfi = orig_score - mod_score
        delta = torch.abs(cfi) / (torch.abs(orig_score) + 1e-8)
        
        # –°–¥–≤–∏–≥ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π
        prob_shift = orig_probs[0, target_class] - mod_probs[0, target_class]
        
        # –ë–∞–∑–æ–≤—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        results = {
            'target_class_analysis': {
                'class_id': target_class,
                'class_name': CLASS_NAMES[target_class],
                'cfi': float(cfi),
                'delta': float(delta),
                'original_score': float(orig_score),
                'modified_score': float(mod_score),
                'original_probability': float(orig_probs[0, target_class]),
                'modified_probability': float(mod_probs[0, target_class]),
                'probability_shift': float(prob_shift),
            },
            'prediction_analysis': {
                'original_prediction': int(orig_pred[0]),
                'original_prediction_name': CLASS_NAMES[int(orig_pred[0])],
                'modified_prediction': int(mod_pred[0]),
                'modified_prediction_name': CLASS_NAMES[int(mod_pred[0])],
                'prediction_changed': bool(orig_pred[0] != mod_pred[0]),
                'original_confidence': float(torch.max(orig_probs)),
                'modified_confidence': float(torch.max(mod_probs)),
                'confidence_drop': float(torch.max(orig_probs) - torch.max(mod_probs))
            }
        }
        
        # –ê–Ω–∞–ª–∏–∑ –≤—Å–µ—Ö –∫–ª–∞—Å—Å–æ–≤ –µ—Å–ª–∏ —Ç—Ä–µ–±—É–µ—Ç—Å—è
        if include_all_classes:
            all_classes_analysis = []
            
            for class_id in range(len(CLASS_NAMES)):
                orig_class_score = classifier.get_per_class_score(original_image, class_id)
                mod_class_score = classifier.get_per_class_score(modified_image, class_id)
                class_cfi = orig_class_score - mod_class_score
                class_delta = torch.abs(class_cfi) / (torch.abs(orig_class_score) + 1e-8)
                
                class_analysis = {
                    'class_id': class_id,
                    'class_name': CLASS_NAMES[class_id],
                    'cfi': float(class_cfi),
                    'delta': float(class_delta),
                    'original_probability': float(orig_probs[0, class_id]),
                    'modified_probability': float(mod_probs[0, class_id]),
                    'probability_shift': float(orig_probs[0, class_id] - mod_probs[0, class_id])
                }
                
                all_classes_analysis.append(class_analysis)
            
            results['all_classes_analysis'] = all_classes_analysis
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        kl_divergence = float(F.kl_div(torch.log(mod_probs + 1e-8), orig_probs, reduction='sum'))
        js_divergence = float(0.5 * (F.kl_div(torch.log((orig_probs + mod_probs)/2 + 1e-8), orig_probs, reduction='sum') + 
                                   F.kl_div(torch.log((orig_probs + mod_probs)/2 + 1e-8), mod_probs, reduction='sum')))
        
        results['distribution_analysis'] = {
            'kl_divergence': kl_divergence,
            'js_divergence': js_divergence,
            'total_variation': float(0.5 * torch.sum(torch.abs(orig_probs - mod_probs)))
        }
    
    return results


print("‚úÖ –§—É–Ω–∫—Ü–∏–∏ –¥–ª—è —Ä–µ–≥–∏–æ–Ω–æ–≤ –∏ –∏–Ω—Ç–µ—Ä–≤–µ–Ω—Ü–∏–π –≥–æ—Ç–æ–≤—ã!")
print(f"üîß –î–æ—Å—Ç—É–ø–Ω—ã–µ —Ç–∏–ø—ã –∏–Ω—Ç–µ—Ä–≤–µ–Ω—Ü–∏–π: noise, gaussian_noise, zero, mean, blur, inpaint, shuffle")
print(f"üìä –ü–æ–¥–¥–µ—Ä–∂–∫–∞ –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏ –≤—Å–µ—Å—Ç–æ—Ä–æ–Ω–Ω–∏–π –∞–Ω–∞–ª–∏–∑ CFI")


def statistical_validation_comprehensive(top_k_shifts, bottom_k_shifts, 
                                       alpha=ALPHA_LEVEL, 
                                       n_bootstrap=N_BOOTSTRAP,
                                       n_permutations=N_PERMUTATIONS):
    """
    –ö–æ–º–ø–ª–µ–∫—Å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è XAI —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    
    –í–∫–ª—é—á–∞–µ—Ç —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –º–µ—Ç–æ–¥—ã:
    - –ü–∞—Ä–∞–º–µ—Ç—Ä–∏—á–µ—Å–∫–∏–µ —Ç–µ—Å—Ç—ã (t-test, Welch's t-test)
    - –ù–µ–ø–∞—Ä–∞–º–µ—Ç—Ä–∏—á–µ—Å–∫–∏–µ —Ç–µ—Å—Ç—ã (Mann-Whitney U, Wilcoxon)
    - Bootstrap confidence intervals
    - Permutation tests
    - Effect size –∞–Ω–∞–ª–∏–∑ (Cohen's d, Cliff's delta)
    - Bayesian analysis (–µ—Å–ª–∏ –≤–æ–∑–º–æ–∂–Ω–æ)
    
    Args:
        top_k_shifts: –º–∞—Å—Å–∏–≤ CFI —Å–¥–≤–∏–≥–æ–≤ –¥–ª—è —Ç–æ–ø-k —Ä–µ–≥–∏–æ–Ω–æ–≤
        bottom_k_shifts: –º–∞—Å—Å–∏–≤ CFI —Å–¥–≤–∏–≥–æ–≤ –¥–ª—è bottom-k —Ä–µ–≥–∏–æ–Ω–æ–≤
        alpha: —É—Ä–æ–≤–µ–Ω—å –∑–Ω–∞—á–∏–º–æ—Å—Ç–∏
        n_bootstrap: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ bootstrap —Å—ç–º–ø–ª–æ–≤
        n_permutations: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–µ—Ä–º—É—Ç–∞—Ü–∏–π
    
    Returns:
        dict: –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    """
    
    print(f"üìä –ü—Ä–æ–≤–µ–¥–µ–Ω–∏–µ –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–æ–π –≤–∞–ª–∏–¥–∞—Ü–∏–∏...")
    print(f"   Top-k –≤—ã–±–æ—Ä–∫–∞: {len(top_k_shifts)} –∑–Ω–∞—á–µ–Ω–∏–π")
    print(f"   Bottom-k –≤—ã–±–æ—Ä–∫–∞: {len(bottom_k_shifts)} –∑–Ω–∞—á–µ–Ω–∏–π")
    print(f"   –£—Ä–æ–≤–µ–Ω—å –∑–Ω–∞—á–∏–º–æ—Å—Ç–∏: Œ± = {alpha}")
    
    # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ numpy
    top_k = np.array(top_k_shifts)
    bottom_k = np.array(bottom_k_shifts)
    
    # –ë–∞–∑–æ–≤—ã–µ –æ–ø–∏—Å–∞—Ç–µ–ª—å–Ω—ã–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
    def compute_descriptive_stats(data, name):
        return {
            'name': name,
            'n': len(data),
            'mean': np.mean(data),
            'median': np.median(data),
            'std': np.std(data, ddof=1),
            'var': np.var(data, ddof=1),
            'min': np.min(data),
            'max': np.max(data),
            'q25': np.percentile(data, 25),
            'q75': np.percentile(data, 75),
            'iqr': np.percentile(data, 75) - np.percentile(data, 25),
            'skewness': stats.skew(data),
            'kurtosis': stats.kurtosis(data),
        }
    
    descriptive_stats = {
        'top_k': compute_descriptive_stats(top_k, 'Top-k'),
        'bottom_k': compute_descriptive_stats(bottom_k, 'Bottom-k')
    }
    
    # 1. –ü–ê–†–ê–ú–ï–¢–†–ò–ß–ï–°–ö–ò–ï –¢–ï–°–¢–´
    parametric_tests = {}
    
    # –û–±—ã—á–Ω—ã–π t-test
    t_stat, t_p_value = stats.ttest_ind(top_k, bottom_k)
    parametric_tests['t_test'] = {
        'statistic': t_stat,
        'p_value': t_p_value,
        'significant': t_p_value < alpha,
        'description': "Independent samples t-test"
    }
    
    # Welch's t-test (–Ω–µ –ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ—Ç —Ä–∞–≤–µ–Ω—Å—Ç–≤–æ –¥–∏—Å–ø–µ—Ä—Å–∏–π)
    welch_t_stat, welch_t_p = stats.ttest_ind(top_k, bottom_k, equal_var=False)
    parametric_tests['welch_t_test'] = {
        'statistic': welch_t_stat,
        'p_value': welch_t_p,
        'significant': welch_t_p < alpha,
        'description': "Welch's t-test (unequal variances)"
    }
    
    # 2. –ù–ï–ü–ê–†–ê–ú–ï–¢–†–ò–ß–ï–°–ö–ò–ï –¢–ï–°–¢–´
    nonparametric_tests = {}
    
    # Mann-Whitney U test
    u_stat, u_p_value = stats.mannwhitneyu(top_k, bottom_k, alternative='two-sided')
    nonparametric_tests['mann_whitney_u'] = {
        'statistic': u_stat,
        'p_value': u_p_value,
        'significant': u_p_value < alpha,
        'description': "Mann-Whitney U test"
    }
    
    # Wilcoxon rank-sum (–∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è)
    try:
        wilcox_stat, wilcox_p = stats.ranksums(top_k, bottom_k)
        nonparametric_tests['wilcoxon_rank_sum'] = {
            'statistic': wilcox_stat,
            'p_value': wilcox_p,
            'significant': wilcox_p < alpha,
            'description': "Wilcoxon rank-sum test"
        }
    except Exception as e:
        print(f"   ‚ö†Ô∏è  Wilcoxon test failed: {e}")
    
    # 3. EFFECT SIZE –ê–ù–ê–õ–ò–ó
    effect_sizes = {}
    
    # Cohen's d
    pooled_std = np.sqrt(((len(top_k) - 1) * np.var(top_k, ddof=1) + 
                         (len(bottom_k) - 1) * np.var(bottom_k, ddof=1)) / 
                        (len(top_k) + len(bottom_k) - 2))
    
    cohens_d = (np.mean(top_k) - np.mean(bottom_k)) / pooled_std if pooled_std > 0 else 0
    
    # –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è Cohen's d
    if abs(cohens_d) < 0.2:
        cohens_interpretation = 'negligible'
    elif abs(cohens_d) < 0.5:
        cohens_interpretation = 'small'
    elif abs(cohens_d) < 0.8:
        cohens_interpretation = 'medium'
    else:
        cohens_interpretation = 'large'
    
    effect_sizes['cohens_d'] = {
        'value': cohens_d,
        'interpretation': cohens_interpretation,
        'description': "Cohen's d (standardized mean difference)"
    }
    
    # Glass's delta (–∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω–∞—è –º–µ—Ä–∞ effect size)
    glass_delta = (np.mean(top_k) - np.mean(bottom_k)) / np.std(bottom_k, ddof=1)
    effect_sizes['glass_delta'] = {
        'value': glass_delta,
        'description': "Glass's delta (using control group std)"
    }
    
    # 4. BOOTSTRAP CONFIDENCE INTERVALS
    def bootstrap_mean_difference(n_bootstrap=n_bootstrap, confidence_level=1-alpha):
        """Bootstrap –æ—Ü–µ–Ω–∫–∞ –¥–æ–≤–µ—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –∏–Ω—Ç–µ—Ä–≤–∞–ª–∞ –¥–ª—è —Ä–∞–∑–Ω–æ—Å—Ç–∏ —Å—Ä–µ–¥–Ω–∏—Ö"""
        
        bootstrap_diffs = []
        for _ in range(n_bootstrap):
            # Resample —Å –≤–æ–∑–≤—Ä–∞—â–µ–Ω–∏–µ–º
            top_sample = np.random.choice(top_k, len(top_k), replace=True)
            bottom_sample = np.random.choice(bottom_k, len(bottom_k), replace=True)
            
            diff = np.mean(top_sample) - np.mean(bottom_sample)
            bootstrap_diffs.append(diff)
        
        bootstrap_diffs = np.array(bootstrap_diffs)
        
        # –î–æ–≤–µ—Ä–∏—Ç–µ–ª—å–Ω—ã–π –∏–Ω—Ç–µ—Ä–≤–∞–ª
        ci_lower = np.percentile(bootstrap_diffs, (1 - confidence_level) / 2 * 100)
        ci_upper = np.percentile(bootstrap_diffs, (1 + confidence_level) / 2 * 100)
        
        return {
            'bootstrap_diffs': bootstrap_diffs,
            'mean_diff': np.mean(bootstrap_diffs),
            'ci_lower': ci_lower,
            'ci_upper': ci_upper,
            'ci_contains_zero': ci_lower <= 0 <= ci_upper,
            'confidence_level': confidence_level
        }
    
    bootstrap_results = bootstrap_mean_difference()
    
    # 5. PERMUTATION TEST
    def permutation_test_comprehensive(n_permutations=n_permutations):
        """–ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –ø–µ—Ä–º—É—Ç–∞—Ü–∏–æ–Ω–Ω—ã–π —Ç–µ—Å—Ç"""
        
        combined = np.concatenate([top_k, bottom_k])
        observed_diff = np.mean(top_k) - np.mean(bottom_k)
        
        permuted_diffs = []
        if len(top_k) >= 2 and len(bottom_k) >= 2:
            for _ in range(n_permutations):
                np.random.shuffle(combined)
                perm_top = combined[:len(top_k)]
                perm_bottom = combined[len(top_k):]
                perm_diff = np.mean(perm_top) - np.mean(perm_bottom)
                permuted_diffs.append(perm_diff)
        else:
            # –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–º—ã—Å–ª–æ–≤–æ–≥–æ –ø–µ—Ä–º—É—Ç–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ —Ç–µ—Å—Ç–∞
            permuted_diffs = np.array([observed_diff])
        
        permuted_diffs = np.array(permuted_diffs)
        
        # Two-tailed p-value
        p_value = np.mean(np.abs(permuted_diffs) >= np.abs(observed_diff)) if permuted_diffs.size > 1 else 1.0
        
        return {
            'observed_difference': observed_diff,
            'permuted_differences': permuted_diffs,
            'p_value': p_value,
            'significant': p_value < alpha,
            'n_permutations': n_permutations
        }
    
    permutation_results = permutation_test_comprehensive()
    
    # 6. –¢–ï–°–¢–´ –ù–ê –ù–û–†–ú–ê–õ–¨–ù–û–°–¢–¨
    normality_tests = {}

    # Shapiro-Wilk test (—Ç—Ä–µ–±—É–µ—Ç n >= 3)
    try:
        if len(top_k) >= 3 and len(bottom_k) >= 3 and len(top_k) <= 5000 and len(bottom_k) <= 5000:
            shapiro_top = stats.shapiro(top_k)
            shapiro_bottom = stats.shapiro(bottom_k)
            normality_tests['shapiro_wilk'] = {
                'top_k': {'statistic': shapiro_top[0], 'p_value': shapiro_top[1], 'normal': shapiro_top[1] > alpha},
                'bottom_k': {'statistic': shapiro_bottom[0], 'p_value': shapiro_bottom[1], 'normal': shapiro_bottom[1] > alpha}
            }
        else:
            normality_tests['shapiro_wilk'] = {
                'top_k': {'skipped': True, 'reason': 'sample_size < 3 or > 5000'},
                'bottom_k': {'skipped': True, 'reason': 'sample_size < 3 or > 5000'}
            }
    except Exception as e:
        normality_tests['shapiro_wilk'] = {'error': str(e)}
    
    # Kolmogorov-Smirnov test
    ks_top = stats.kstest(top_k, 'norm', args=(np.mean(top_k), np.std(top_k)))
    ks_bottom = stats.kstest(bottom_k, 'norm', args=(np.mean(bottom_k), np.std(bottom_k)))
    
    normality_tests['kolmogorov_smirnov'] = {
        'top_k': {'statistic': ks_top[0], 'p_value': ks_top[1], 'normal': ks_top[1] > alpha},
        'bottom_k': {'statistic': ks_bottom[0], 'p_value': ks_bottom[1], 'normal': ks_bottom[1] > alpha}
    }
    
    # 7. –¢–ï–°–¢–´ –ù–ê –†–ê–í–ï–ù–°–¢–í–û –î–ò–°–ü–ï–†–°–ò–ô
    variance_tests = {}
    
    # Levene's test
    levene_stat, levene_p = stats.levene(top_k, bottom_k)
    variance_tests['levene'] = {
        'statistic': levene_stat,
        'p_value': levene_p,
        'equal_variances': levene_p > alpha,
        'description': "Levene's test for equal variances"
    }
    
    # F-test
    f_stat = np.var(top_k, ddof=1) / np.var(bottom_k, ddof=1)
    f_p_value = 2 * min(stats.f.cdf(f_stat, len(top_k)-1, len(bottom_k)-1),
                       1 - stats.f.cdf(f_stat, len(top_k)-1, len(bottom_k)-1))
    
    variance_tests['f_test'] = {
        'statistic': f_stat,
        'p_value': f_p_value,
        'equal_variances': f_p_value > alpha,
        'description': "F-test for equal variances"
    }
    
    # 8. –ò–¢–û–ì–û–í–´–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´
    # –ö–æ–Ω—Å–µ–Ω—Å—É—Å –ø–æ –∑–Ω–∞—á–∏–º–æ—Å—Ç–∏
    significance_consensus = {
        'parametric_significant': any([test['significant'] for test in parametric_tests.values()]),
        'nonparametric_significant': any([test['significant'] for test in nonparametric_tests.values()]),
        'bootstrap_significant': not bootstrap_results['ci_contains_zero'],
        'permutation_significant': permutation_results['significant']
    }
    
    # –û–±—â–∏–π –∫–æ–Ω—Å–µ–Ω—Å—É—Å
    total_significant_tests = sum(significance_consensus.values())
    consensus_threshold = len(significance_consensus) // 2 + 1  # –ë–æ–ª—å—à–µ –ø–æ–ª–æ–≤–∏–Ω—ã
    
    overall_significant = total_significant_tests >= consensus_threshold
    
    # –ö–æ–º–ø–∏–ª—è—Ü–∏—è —Ñ–∏–Ω–∞–ª—å–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    final_results = {
        'descriptive_statistics': descriptive_stats,
        'parametric_tests': parametric_tests,
        'nonparametric_tests': nonparametric_tests,
        'effect_sizes': effect_sizes,
        'bootstrap_analysis': bootstrap_results,
        'permutation_analysis': permutation_results,
        'normality_tests': normality_tests,
        'variance_tests': variance_tests,
        'significance_consensus': significance_consensus,
        'overall_conclusion': {
            'significant': overall_significant,
            'significant_tests_count': total_significant_tests,
            'total_tests_count': len(significance_consensus),
            'alpha_level': alpha,
            'recommendation': 'significant' if overall_significant else 'not_significant'
        },
        'metadata': {
            'analysis_timestamp': datetime.now().isoformat(),
            'n_bootstrap_samples': n_bootstrap,
            'n_permutations': n_permutations,
            'alpha_level': alpha
        }
    }
    
    print(f"   ‚úÖ –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à—ë–Ω. –ó–Ω–∞—á–∏–º—ã—Ö —Ç–µ—Å—Ç–æ–≤: {total_significant_tests}/{len(significance_consensus)}")
    print(f"   üìä –û–±—â–∏–π –≤—ã–≤–æ–¥: {'–ó–ù–ê–ß–ò–ú–û' if overall_significant else '–ù–ï –ó–ù–ê–ß–ò–ú–û'} (Œ± = {alpha})")
    
    return final_results


def sanity_check_comprehensive(classifier, test_image, target_class, xai_analyzer, 
                             n_trials=3, randomization_strength=0.01):
    """
    –ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–π sanity check –¥–ª—è XAI –º–µ—Ç–æ–¥–æ–≤
    
    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç —á—Ç–æ XAI –∫–∞—Ä—Ç—ã –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ –æ—Ç—Ä–∞–∂–∞—é—Ç —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏:
    1. –†–∞–Ω–¥–æ–º–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤ –¥–æ–ª–∂–Ω–∞ —Ä–∞–∑—Ä—É—à–∏—Ç—å –∫–∞—Ä—Ç—ã
    2. –ù–µ–∑–∞–≤–∏—Å–∏–º—ã–µ –≤—Ö–æ–¥—ã –¥–æ–ª–∂–Ω—ã –¥–∞–≤–∞—Ç—å –Ω–µ–∑–∞–≤–∏—Å–∏–º—ã–µ –∫–∞—Ä—Ç—ã
    3. –ö–∞—Ä—Ç—ã –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã –∫ –∏–∑–º–µ–Ω–µ–Ω–∏—è–º –º–æ–¥–µ–ª–∏
    
    Args:
        classifier: –º–æ–¥–µ–ª—å –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞
        test_image: —Ç–µ—Å—Ç–æ–≤–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
        target_class: —Ü–µ–ª–µ–≤–æ–π –∫–ª–∞—Å—Å
        xai_analyzer: XAI –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä
        n_trials: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏—Å–ø—ã—Ç–∞–Ω–∏–π
        randomization_strength: —Å–∏–ª–∞ —Ä–∞–Ω–¥–æ–º–∏–∑–∞—Ü–∏–∏ –≤–µ—Å–æ–≤
    
    Returns:
        dict: —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã sanity checks
    """
    
    print("üîç –ü—Ä–æ–≤–µ–¥–µ–Ω–∏–µ –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–≥–æ sanity check...")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ –≤–µ—Å–∞
    original_state = {name: param.clone() for name, param in classifier.named_parameters()}
    
    results = {
        'weight_randomization_test': {},
        'input_independence_test': {},
        'model_sensitivity_test': {},
        'overall_sanity_score': 0.0
    }
    
    try:
        # 1. –¢–ï–°–¢ –†–ê–ù–î–û–ú–ò–ó–ê–¶–ò–ò –í–ï–°–û–í
        print("   üé≤ –¢–µ—Å—Ç —Ä–∞–Ω–¥–æ–º–∏–∑–∞—Ü–∏–∏ –≤–µ—Å–æ–≤...")
        
        # –í—ã—á–∏—Å–ª—è–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ –∫–∞—Ä—Ç—ã –≤–∞–∂–Ω–æ—Å—Ç–∏
        original_attribution = xai_analyzer.compute_integrated_gradients(
            test_image, target_class, n_steps=20
        )
        
        correlations_with_random = []
        
        for trial in range(n_trials):
            # –†–∞–Ω–¥–æ–º–∏–∑–∏—Ä—É–µ–º –≤–µ—Å–∞
            with torch.no_grad():
                for name, param in classifier.named_parameters():
                    if param.dim() > 1:  # –¢–æ–ª—å–∫–æ –≤–µ—Å–∞, –Ω–µ bias
                        random_weights = torch.randn_like(param) * randomization_strength
                        param.data = random_weights
            
            # –í—ã—á–∏—Å–ª—è–µ–º –∫–∞—Ä—Ç—ã —Å —Ä–∞–Ω–¥–æ–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª—å—é
            try:
                randomized_attribution = xai_analyzer.compute_integrated_gradients(
                    test_image, target_class, n_steps=20
                )
                
                # –ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è –º–µ–∂–¥—É –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–π –∏ —Ä–∞–Ω–¥–æ–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –∫–∞—Ä—Ç–∞–º–∏
                orig_flat = original_attribution.flatten().detach().cpu().numpy()
                rand_flat = randomized_attribution.flatten().detach().cpu().numpy()
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ NaN –∏ –±–µ—Å–∫–æ–Ω–µ—á–Ω–æ—Å—Ç—å
                if np.any(np.isnan(orig_flat)) or np.any(np.isnan(rand_flat)):
                    correlation = 0.0
                else:
                    correlation = np.corrcoef(orig_flat, rand_flat)[0, 1]
                    if np.isnan(correlation):
                        correlation = 0.0
                
                correlations_with_random.append(abs(correlation))
                
            except Exception as e:
                print(f"      ‚ö†Ô∏è  –û—à–∏–±–∫–∞ –≤ trial {trial}: {e}")
                correlations_with_random.append(0.0)
        
        mean_random_correlation = np.mean(correlations_with_random)
        random_test_passed = mean_random_correlation < 0.1  # –ü–æ—Ä–æ–≥ –¥–ª—è sanity check
        
        results['weight_randomization_test'] = {
            'mean_correlation_with_random': mean_random_correlation,
            'correlations_per_trial': correlations_with_random,
            'test_passed': random_test_passed,
            'threshold': 0.1,
            'n_trials': n_trials
        }
        
        # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ –≤–µ—Å–∞
        for name, param in classifier.named_parameters():
            param.data = original_state[name].data
        
        # 2. –¢–ï–°–¢ –ù–ï–ó–ê–í–ò–°–ò–ú–û–°–¢–ò –í–•–û–î–û–í
        print("   üîÑ –¢–µ—Å—Ç –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –≤—Ö–æ–¥–æ–≤...")
        
        # –°–æ–∑–¥–∞—ë–º –Ω–µ–∑–∞–≤–∏—Å–∏–º—ã–µ –≤—Ö–æ–¥—ã
        independent_inputs = []
        for _ in range(3):
            noise_input = torch.randn_like(test_image)
            independent_inputs.append(noise_input)
        
        # –í—ã—á–∏—Å–ª—è–µ–º –∫–∞—Ä—Ç—ã –¥–ª—è –Ω–µ–∑–∞–≤–∏—Å–∏–º—ã—Ö –≤—Ö–æ–¥–æ–≤
        independent_attributions = []
        for inp in independent_inputs:
            try:
                attr = xai_analyzer.compute_integrated_gradients(inp, target_class, n_steps=15)
                independent_attributions.append(attr.flatten().detach().cpu().numpy())
            except Exception as e:
                print(f"      ‚ö†Ô∏è  –û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã—á–∏—Å–ª–µ–Ω–∏–∏ –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ–π –∞—Ç—Ä–∏–±—É—Ü–∏–∏: {e}")
                continue
        
        # –ö–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ –º–µ–∂–¥—É –Ω–µ–∑–∞–≤–∏—Å–∏–º—ã–º–∏ –∫–∞—Ä—Ç–∞–º–∏
        independence_correlations = []
        if len(independent_attributions) >= 2:
            for i in range(len(independent_attributions)):
                for j in range(i + 1, len(independent_attributions)):
                    corr = np.corrcoef(independent_attributions[i], independent_attributions[j])[0, 1]
                    if not np.isnan(corr):
                        independence_correlations.append(abs(corr))
        
        mean_independence_correlation = np.mean(independence_correlations) if independence_correlations else 0.0
        independence_test_passed = mean_independence_correlation < 0.3
        
        results['input_independence_test'] = {
            'mean_correlation_between_independent': mean_independence_correlation,
            'independence_correlations': independence_correlations,
            'test_passed': independence_test_passed,
            'threshold': 0.3,
            'n_independent_inputs': len(independent_inputs)
        }
        
        # 3. –¢–ï–°–¢ –ß–£–í–°–¢–í–ò–¢–ï–õ–¨–ù–û–°–¢–ò –ú–û–î–ï–õ–ò
        print("   üéØ –¢–µ—Å—Ç —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏...")
        
        # –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º –∫–∞—Ä—Ç—ã –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –∫–ª–∞—Å—Å–æ–≤
        different_class_correlations = []
        
        for other_class in range(min(3, len(CLASS_NAMES))):
            if other_class != target_class:
                try:
                    other_class_attr = xai_analyzer.compute_integrated_gradients(
                        test_image, other_class, n_steps=15
                    )
                    
                    orig_flat = original_attribution.flatten().detach().cpu().numpy()
                    other_flat = other_class_attr.flatten().detach().cpu().numpy()
                    
                    corr = np.corrcoef(orig_flat, other_flat)[0, 1]
                    if not np.isnan(corr):
                        different_class_correlations.append(abs(corr))
                        
                except Exception as e:
                    print(f"      ‚ö†Ô∏è  –û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ –∫–ª–∞—Å—Å–∞ {other_class}: {e}")
                    continue
        
        mean_different_class_correlation = np.mean(different_class_correlations) if different_class_correlations else 1.0
        sensitivity_test_passed = mean_different_class_correlation < 0.8  # –ö–∞—Ä—Ç—ã –¥–æ–ª–∂–Ω—ã —Ä–∞–∑–ª–∏—á–∞—Ç—å—Å—è
        
        results['model_sensitivity_test'] = {
            'mean_correlation_different_classes': mean_different_class_correlation,
            'different_class_correlations': different_class_correlations,
            'test_passed': sensitivity_test_passed,
            'threshold': 0.8,
            'classes_tested': len(different_class_correlations)
        }
        
        # –û–ë–©–ò–ô SANITY SCORE
        passed_tests = [
            results['weight_randomization_test']['test_passed'],
            results['input_independence_test']['test_passed'],
            results['model_sensitivity_test']['test_passed']
        ]
        
        sanity_score = sum(passed_tests) / len(passed_tests)
        results['overall_sanity_score'] = sanity_score
        
        # –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è
        if sanity_score >= 0.67:
            sanity_interpretation = 'good'
        elif sanity_score >= 0.33:
            sanity_interpretation = 'moderate'
        else:
            sanity_interpretation = 'poor'
        
        results['overall_interpretation'] = sanity_interpretation
        
        print(f"   üìä Sanity score: {sanity_score:.2f} ({sanity_interpretation})")
        print(f"   ‚úÖ –ü—Ä–æ–π–¥–µ–Ω–Ω—ã—Ö —Ç–µ—Å—Ç–æ–≤: {sum(passed_tests)}/{len(passed_tests)}")
        
    except Exception as e:
        print(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –≤ sanity check: {e}")
        results['error'] = str(e)
        
    finally:
        # –û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û –≤–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ –≤–µ—Å–∞
        try:
            for name, param in classifier.named_parameters():
                param.data = original_state[name].data
            print("   üîÑ –û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ –≤–µ—Å–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã")
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è –≤–µ—Å–æ–≤: {e}")
    
    return results


print("‚úÖ –ö–æ–º–ø–ª–µ–∫—Å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è –≥–æ—Ç–æ–≤–∞!")
print("üìä –í–∫–ª—é—á–∞–µ—Ç: –ø–∞—Ä–∞–º–µ—Ç—Ä–∏—á–µ—Å–∫–∏–µ/–Ω–µ–ø–∞—Ä–∞–º–µ—Ç—Ä–∏—á–µ—Å–∫–∏–µ —Ç–µ—Å—Ç—ã, bootstrap, permutation, effect size")
print("üîç Sanity checks: —Ä–∞–Ω–¥–æ–º–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤, –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å –≤—Ö–æ–¥–æ–≤, —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏")


def tensor_to_displayable_image(tensor, denormalize=True):
    """–ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç —Ç–µ–Ω–∑–æ—Ä PyTorch –≤ numpy array –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è"""
    
    if torch.is_tensor(tensor):
        img = tensor.squeeze().detach().cpu().numpy()
    else:
        img = tensor
    
    # –ü–µ—Ä–µ—Å—Ç–∞–≤–ª—è–µ–º –æ—Å–∏ –µ—Å–ª–∏ –Ω—É–∂–Ω–æ (CHW -> HWC)
    if len(img.shape) == 3 and img.shape[0] in [1, 3]:
        img = np.transpose(img, (1, 2, 0))
    
    # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω—é—é —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –¥–ª—è grayscale
    if len(img.shape) == 3 and img.shape[2] == 1:
        img = img.squeeze(axis=2)
    
    # –î–µ–Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∏–∑ [-1, 1] –≤ [0, 1]
    if denormalize:
        img = (img + 1.0) / 2.0
    
    return np.clip(img, 0, 1)


def visualize_xai_step_comprehensive(image, attribution_map, top_k_mask, bottom_k_mask,
                                   timestep, class_name, save_path=None, figsize=(20, 5)):
    """
    –ö–æ–º–ø–ª–µ–∫—Å–Ω–∞—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è XAI –∞–Ω–∞–ª–∏–∑–∞ –¥–ª—è –æ–¥–Ω–æ–≥–æ –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ —à–∞–≥–∞
    
    –ü–æ–∫–∞–∑—ã–≤–∞–µ—Ç: –æ—Ä–∏–≥–∏–Ω–∞–ª, –∫–∞—Ä—Ç—É –≤–∞–∂–Ω–æ—Å—Ç–∏, —Ç–æ–ø-k –∏ bottom-k –º–∞—Å–∫–∏, –Ω–∞–ª–æ–∂–µ–Ω–∏—è
    """
    
    fig, axes = plt.subplots(1, 5, figsize=figsize)
    
    # 1. –û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
    img_display = tensor_to_displayable_image(image)
    axes[0].imshow(img_display, cmap='gray' if len(img_display.shape) == 2 else None)
    axes[0].set_title(f'Original\nt = {timestep:.0f}', fontsize=12, weight='bold')
    axes[0].axis('off')
    
    # 2. –ö–∞—Ä—Ç–∞ –∞—Ç—Ä–∏–±—É—Ü–∏–∏
    if torch.is_tensor(attribution_map):
        attr_display = attribution_map.squeeze().detach().cpu().numpy()
        if len(attr_display.shape) == 3:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º L2 –Ω–æ—Ä–º—É –ø–æ –∫–∞–Ω–∞–ª–∞–º
            attr_display = np.linalg.norm(attr_display, axis=0)
        else:
            attr_display = np.abs(attr_display)
    else:
        attr_display = np.abs(attribution_map)
    
    im1 = axes[1].imshow(attr_display, cmap='hot', alpha=0.8)
    axes[1].set_title(f'Attribution Map\n(max: {attr_display.max():.3f})', fontsize=12)
    axes[1].axis('off')
    plt.colorbar(im1, ax=axes[1], shrink=0.8, aspect=20)
    
    # 3. –¢–æ–ø-k —Ä–µ–≥–∏–æ–Ω—ã
    axes[2].imshow(img_display, cmap='gray' if len(img_display.shape) == 2 else None)
    axes[2].imshow(top_k_mask, cmap='Reds', alpha=0.6)
    top_k_coverage = np.sum(top_k_mask) / top_k_mask.size * 100
    axes[2].set_title(f'Top-k Regions\n({top_k_coverage:.1f}% coverage)', fontsize=12)
    axes[2].axis('off')
    
    # 4. Bottom-k —Ä–µ–≥–∏–æ–Ω—ã
    axes[3].imshow(img_display, cmap='gray' if len(img_display.shape) == 2 else None)
    axes[3].imshow(bottom_k_mask, cmap='Blues', alpha=0.6)
    bottom_k_coverage = np.sum(bottom_k_mask) / bottom_k_mask.size * 100
    axes[3].set_title(f'Bottom-k Regions\n({bottom_k_coverage:.1f}% coverage)', fontsize=12)
    axes[3].axis('off')
    
    # 5. –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –Ω–∞–ª–æ–∂–µ–Ω–∏–µ
    axes[4].imshow(img_display, cmap='gray' if len(img_display.shape) == 2 else None)
    axes[4].imshow(top_k_mask, cmap='Reds', alpha=0.4)
    axes[4].imshow(bottom_k_mask, cmap='Blues', alpha=0.3)
    axes[4].set_title(f'{class_name} XAI\nRed: Top-k, Blue: Bottom-k', fontsize=12)
    axes[4].axis('off')
    
    plt.suptitle(f'üî¨ XAI Analysis Step: {class_name} at timestep {timestep:.0f}', 
                fontsize=16, y=1.02, weight='bold')
    plt.tight_layout()
    
    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight', facecolor='white')
        print(f"   üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {save_path}")
    
    plt.show()


def visualize_intervention_comprehensive(original_image, masks_dict, interventions_dict,
                                       cfi_results_dict, timestep=None, save_path=None):
    """
    –ö–æ–º–ø–ª–µ–∫—Å–Ω–∞—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∏–Ω—Ç–µ—Ä–≤–µ–Ω—Ü–∏–π
    
    Args:
        original_image: –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
        masks_dict: —Å–ª–æ–≤–∞—Ä—å –º–∞—Å–æ–∫ {'top_k': mask, 'bottom_k': mask}
        interventions_dict: —Å–ª–æ–≤–∞—Ä—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∏–Ω—Ç–µ—Ä–≤–µ–Ω—Ü–∏–π
        cfi_results_dict: —Å–ª–æ–≤–∞—Ä—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ CFI
    """
    
    n_interventions = len(interventions_dict)
    n_cols = min(4, n_interventions + 1)  # +1 –¥–ª—è –æ—Ä–∏–≥–∏–Ω–∞–ª–∞
    n_rows = (n_interventions + n_cols) // n_cols
    
    fig, axes = plt.subplots(n_rows, n_cols, figsize=(5 * n_cols, 4 * n_rows))
    if n_rows == 1:
        axes = axes.reshape(1, -1)
    
    # –ò—Å—Ö–æ–¥–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
    img_display = tensor_to_displayable_image(original_image)
    axes[0, 0].imshow(img_display, cmap='gray' if len(img_display.shape) == 2 else None)
    axes[0, 0].set_title('Original', fontsize=12, weight='bold')
    axes[0, 0].axis('off')
    
    plot_idx = 1
    
    for region_type, intervention_results in interventions_dict.items():
        for intervention_type, result_data in intervention_results.items():
            row = plot_idx // n_cols
            col = plot_idx % n_cols
            
            if row >= n_rows:
                break
            
            # –ú–æ–¥–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            modified_img = tensor_to_displayable_image(result_data['modified_image'])
            axes[row, col].imshow(modified_img, cmap='gray' if len(modified_img.shape) == 2 else None)
            
            # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞—Ö
            cfi_key = f"{region_type}_{intervention_type}"
            if cfi_key in cfi_results_dict:
                cfi_result = cfi_results_dict[cfi_key]
                cfi_val = cfi_result['target_class_analysis']['cfi']
                pred_changed = cfi_result['prediction_analysis']['prediction_changed']
                
                title = f"{region_type.replace('_', '-').title()}\n{intervention_type.title()}\n"
                title += f"CFI: {cfi_val:.3f}\nPred: {'‚úì' if pred_changed else '‚úó'}"
            else:
                title = f"{region_type.replace('_', '-').title()}\n{intervention_type.title()}"
            
            axes[row, col].set_title(title, fontsize=10)
            axes[row, col].axis('off')
            
            plot_idx += 1
    
    # –£–¥–∞–ª—è–µ–º –Ω–µ–∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–µ –ø–æ–¥–≥—Ä–∞—Ñ–∏–∫–∏
    for idx in range(plot_idx, n_rows * n_cols):
        row = idx // n_cols
        col = idx % n_cols
        if row < n_rows and col < n_cols:
            axes[row, col].axis('off')
    
    title = f'üß™ Counterfactual Interventions'
    if timestep is not None:
        title += f' (t = {timestep:.0f})'
    
    plt.suptitle(title, fontsize=16, y=0.98, weight='bold')
    plt.tight_layout()
    
    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight', facecolor='white')
    
    plt.show()


def plot_time_shap_comprehensive(timesteps, time_importance, time_data, class_name, save_path=None):
    """
    –ö–æ–º–ø–ª–µ–∫—Å–Ω–∞—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è Time-SHAP –∞–Ω–∞–ª–∏–∑–∞
    """
    
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))
    
    # 1. –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è –≤–∞–∂–Ω–æ—Å—Ç—å –ø–æ –≤—Ä–µ–º–µ–Ω–∏
    ax1.plot(timesteps, time_importance, 'bo-', linewidth=3, markersize=8, alpha=0.7)
    ax1.fill_between(timesteps, time_importance, alpha=0.3, color='blue')
    
    # –û—Ç–º–µ—á–∞–µ–º –Ω–∞–∏–±–æ–ª–µ–µ –≤–∞–∂–Ω—ã–π —à–∞–≥
    max_idx = np.argmax(time_importance)
    ax1.axvline(x=timesteps[max_idx], color='red', linestyle='--', alpha=0.8, linewidth=2,
               label=f'Most important: t={timesteps[max_idx]:.0f}')
    ax1.scatter(timesteps[max_idx], time_importance[max_idx], 
               color='red', s=150, zorder=10, edgecolor='darkred', linewidth=2)
    
    ax1.set_xlabel('Timestep t', fontsize=12)
    ax1.set_ylabel('Normalized Importance', fontsize=12)
    ax1.set_title(f'Time-SHAP: Temporal Importance for {class_name}', fontsize=14, weight='bold')
    ax1.grid(True, alpha=0.3)
    ax1.legend()
    
    # 2. Per-class scores (–ª–æ–≥–∞—Ä–∏—Ñ–º–∏—á–µ—Å–∫–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏)
    confidence_scores = time_data['confidence_scores']
    ax2.plot(timesteps, confidence_scores, 'go-', linewidth=3, markersize=8, alpha=0.7)
    ax2.fill_between(timesteps, confidence_scores, alpha=0.3, color='green')
    
    ax2.axvline(x=timesteps[max_idx], color='red', linestyle='--', alpha=0.8, linewidth=2)
    ax2.scatter(timesteps[max_idx], confidence_scores[max_idx], 
               color='red', s=150, zorder=10, edgecolor='darkred', linewidth=2)
    
    ax2.set_xlabel('Timestep t', fontsize=12)
    ax2.set_ylabel('Log Probability Score', fontsize=12)
    ax2.set_title('Per-class Score Evolution', fontsize=14, weight='bold')
    ax2.grid(True, alpha=0.3)
    
    # 3. –û–±—ã—á–Ω—ã–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏
    probability_scores = time_data['probability_scores']
    ax3.plot(timesteps, probability_scores, 'mo-', linewidth=3, markersize=8, alpha=0.7)
    ax3.fill_between(timesteps, probability_scores, alpha=0.3, color='magenta')
    
    ax3.axvline(x=timesteps[max_idx], color='red', linestyle='--', alpha=0.8, linewidth=2)
    ax3.scatter(timesteps[max_idx], probability_scores[max_idx], 
               color='red', s=150, zorder=10, edgecolor='darkred', linewidth=2)
    
    ax3.set_xlabel('Timestep t', fontsize=12)
    ax3.set_ylabel('Probability', fontsize=12)
    ax3.set_title('Probability Evolution', fontsize=14, weight='bold')
    ax3.grid(True, alpha=0.3)
    
    # 4. –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –≤–∞–∂–Ω–æ—Å—Ç–∏
    ax4.hist(time_importance, bins=10, alpha=0.7, color='skyblue', edgecolor='black')
    ax4.axvline(x=np.mean(time_importance), color='red', linestyle='-', 
               label=f'Mean: {np.mean(time_importance):.3f}', linewidth=2)
    ax4.axvline(x=np.median(time_importance), color='orange', linestyle='--', 
               label=f'Median: {np.median(time_importance):.3f}', linewidth=2)
    
    ax4.set_xlabel('Importance Value', fontsize=12)
    ax4.set_ylabel('Frequency', fontsize=12)
    ax4.set_title('Distribution of Time Importance', fontsize=14, weight='bold')
    ax4.legend()
    ax4.grid(True, alpha=0.3)
    
    plt.suptitle(f'üïí Time-SHAP Comprehensive Analysis: {class_name}', 
                fontsize=18, y=0.98, weight='bold')
    plt.tight_layout()
    
    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight', facecolor='white')
    
    plt.show()


def plot_statistical_analysis_modern(statistical_results, class_name, save_path=None):
    """
    –°–æ–≤—Ä–µ–º–µ–Ω–Ω–∞—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
    """
    
    fig = plt.figure(figsize=(20, 16))
    gs = fig.add_gridspec(4, 3, hspace=0.3, wspace=0.3)
    
    # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞–Ω–Ω—ã–µ
    top_k_stats = statistical_results['descriptive_statistics']['top_k']
    bottom_k_stats = statistical_results['descriptive_statistics']['bottom_k']
    
    # –°–æ–∑–¥–∞—ë–º —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫
    np.random.seed(42)
    top_k_synthetic = np.random.normal(top_k_stats['mean'], top_k_stats['std'], top_k_stats['n'])
    bottom_k_synthetic = np.random.normal(bottom_k_stats['mean'], bottom_k_stats['std'], bottom_k_stats['n'])
    
    # 1. –ö–æ—Ä–æ–±—á–∞—Ç—ã–µ –¥–∏–∞–≥—Ä–∞–º–º—ã
    ax1 = fig.add_subplot(gs[0, 0])
    data_to_plot = [top_k_synthetic, bottom_k_synthetic]
    box_plot = ax1.boxplot(data_to_plot, labels=['Top-k', 'Bottom-k'], patch_artist=True)
    box_plot['boxes'][0].set_facecolor('lightcoral')
    box_plot['boxes'][1].set_facecolor('lightblue')
    ax1.set_ylabel('Causal Shift (CFI)', fontsize=12)
    ax1.set_title('CFI Distribution Comparison', fontsize=14, weight='bold')
    ax1.grid(True, alpha=0.3)
    
    # 2. –ì–∏—Å—Ç–æ–≥—Ä–∞–º–º—ã
    ax2 = fig.add_subplot(gs[0, 1])
    ax2.hist(top_k_synthetic, alpha=0.7, label='Top-k', bins=20, color='lightcoral', density=True)
    ax2.hist(bottom_k_synthetic, alpha=0.7, label='Bottom-k', bins=20, color='lightblue', density=True)
    ax2.set_xlabel('CFI Value', fontsize=12)
    ax2.set_ylabel('Density', fontsize=12)
    ax2.set_title('Probability Density Functions', fontsize=14, weight='bold')
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    
    # 3. P-values comparison
    ax3 = fig.add_subplot(gs[0, 2])
    tests = ['t-test', 'Welch t-test', 'Mann-Whitney', 'Permutation']
    p_values = [
        statistical_results['parametric_tests']['t_test']['p_value'],
        statistical_results['parametric_tests']['welch_t_test']['p_value'],
        statistical_results['nonparametric_tests']['mann_whitney_u']['p_value'],
        statistical_results['permutation_analysis']['p_value']
    ]
    
    colors = ['coral', 'orange', 'skyblue', 'lightgreen']
    bars = ax3.bar(tests, p_values, color=colors, alpha=0.8, edgecolor='black')
    ax3.axhline(y=0.05, color='red', linestyle='--', linewidth=2, label='Œ± = 0.05')
    ax3.axhline(y=0.01, color='darkred', linestyle=':', linewidth=2, label='Œ± = 0.01')
    
    ax3.set_ylabel('p-value', fontsize=12)
    ax3.set_title('Statistical Test Results', fontsize=14, weight='bold')
    ax3.set_yscale('log')
    ax3.legend()
    ax3.grid(True, alpha=0.3)
    plt.setp(ax3.xaxis.get_majorticklabels(), rotation=45)
    
    # –î–æ–±–∞–≤–ª—è–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –Ω–∞ —Å—Ç–æ–ª–±—Ü—ã
    for bar, p_val in zip(bars, p_values):
        height = bar.get_height()
        ax3.text(bar.get_x() + bar.get_width()/2., height * 1.1,
                f'{p_val:.1e}', ha='center', va='bottom', fontsize=9, rotation=45)
    
    # 4. Bootstrap confidence interval
    ax4 = fig.add_subplot(gs[1, 0])
    bootstrap_data = statistical_results['bootstrap_analysis']
    mean_diff = bootstrap_data['mean_diff']
    ci_lower = bootstrap_data['ci_lower']
    ci_upper = bootstrap_data['ci_upper']
    
    ax4.errorbar([0], [mean_diff], yerr=[[mean_diff - ci_lower], [ci_upper - mean_diff]], 
                fmt='ro', capsize=15, markersize=10, capthick=3, linewidth=3)
    ax4.axhline(y=0, color='black', linestyle='-', alpha=0.5, linewidth=2)
    ax4.set_xlim(-0.5, 0.5)
    ax4.set_ylabel('Mean Difference', fontsize=12)
    ax4.set_title(f'Bootstrap 95% CI\nContains 0: {bootstrap_data["ci_contains_zero"]}', fontsize=14, weight='bold')
    ax4.grid(True, alpha=0.3)
    ax4.set_xticks([])
    
    # 5. Effect sizes
    ax5 = fig.add_subplot(gs[1, 1])
    effect_sizes = statistical_results['effect_sizes']
    effect_names = list(effect_sizes.keys())
    effect_values = [effect_sizes[name]['value'] for name in effect_names]
    
    bars_effect = ax5.barh(effect_names, effect_values, color='mediumpurple', alpha=0.7, edgecolor='black')
    ax5.axvline(x=0, color='black', linestyle='-', alpha=0.5)
    ax5.axvline(x=0.2, color='orange', linestyle='--', alpha=0.7, label='Small effect')
    ax5.axvline(x=0.5, color='red', linestyle='--', alpha=0.7, label='Medium effect')
    ax5.axvline(x=0.8, color='darkred', linestyle='--', alpha=0.7, label='Large effect')
    
    ax5.set_xlabel('Effect Size', fontsize=12)
    ax5.set_title('Effect Size Analysis', fontsize=14, weight='bold')
    ax5.legend(loc='lower right')
    ax5.grid(True, alpha=0.3)
    
    # 6. Test significance summary
    ax6 = fig.add_subplot(gs[1, 2])
    ax6.axis('off')
    
    # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º –∑–Ω–∞—á–∏–º—ã–µ —Ç–µ—Å—Ç—ã
    significance_data = statistical_results['significance_consensus']
    significant_count = sum(significance_data.values())
    total_tests = len(significance_data)
    
    # –°–æ–∑–¥–∞—ë–º –∫—Ä—É–≥–æ–≤—É—é –¥–∏–∞–≥—Ä–∞–º–º—É –∑–Ω–∞—á–∏–º–æ—Å—Ç–∏
    labels = ['Significant', 'Not Significant']
    sizes = [significant_count, total_tests - significant_count]
    colors = ['lightgreen', 'lightcoral']
    
    wedges, texts, autotexts = ax6.pie(sizes, labels=labels, colors=colors, autopct='%1.0f%%',
                                      startangle=90, textprops={'fontsize': 12})
    ax6.set_title(f'Test Significance Summary\n({significant_count}/{total_tests} significant)', 
                 fontsize=14, weight='bold')
    
    # 7-12. –î–µ—Ç–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∞—è —Ç–∞–±–ª–∏—Ü–∞
    ax7 = fig.add_subplot(gs[2:, :])
    ax7.axis('off')
    
    # –°–æ–∑–¥–∞–µ–º –¥–µ—Ç–∞–ª—å–Ω—É—é —Ç–∞–±–ª–∏—Ü—É
    table_data = []
    
    # –ë–∞–∑–æ–≤—ã–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
    table_data.append(['Statistic', 'Top-k', 'Bottom-k', 'Difference'])
    table_data.append(['Sample Size', f"{top_k_stats['n']}", f"{bottom_k_stats['n']}", '‚Äî'])
    table_data.append(['Mean', f"{top_k_stats['mean']:.4f}", f"{bottom_k_stats['mean']:.4f}", 
                      f"{top_k_stats['mean'] - bottom_k_stats['mean']:.4f}"])
    table_data.append(['Std Dev', f"{top_k_stats['std']:.4f}", f"{bottom_k_stats['std']:.4f}", '‚Äî'])
    table_data.append(['Median', f"{top_k_stats['median']:.4f}", f"{bottom_k_stats['median']:.4f}", 
                      f"{top_k_stats['median'] - bottom_k_stats['median']:.4f}"])
    table_data.append(['IQR', f"{top_k_stats['iqr']:.4f}", f"{bottom_k_stats['iqr']:.4f}", '‚Äî'])
    
    # –¢–µ—Å—Ç—ã –∑–Ω–∞—á–∏–º–æ—Å—Ç–∏
    table_data.append(['', '', '', ''])  # –ü—É—Å—Ç–∞—è —Å—Ç—Ä–æ–∫–∞
    table_data.append(['Test', 'Statistic', 'p-value', 'Significant'])
    
    for test_category in ['parametric_tests', 'nonparametric_tests']:
        for test_name, test_result in statistical_results[test_category].items():
            formatted_name = test_name.replace('_', ' ').title()
            table_data.append([
                formatted_name,
                f"{test_result['statistic']:.4f}",
                f"{test_result['p_value']:.1e}",
                '‚úÖ' if test_result['significant'] else '‚ùå'
            ])
    
    # Permutation test
    perm_result = statistical_results['permutation_analysis']
    table_data.append([
        'Permutation Test',
        f"{perm_result['observed_difference']:.4f}",
        f"{perm_result['p_value']:.1e}",
        '‚úÖ' if perm_result['significant'] else '‚ùå'
    ])
    
    # Effect sizes
    table_data.append(['', '', '', ''])  # –ü—É—Å—Ç–∞—è —Å—Ç—Ä–æ–∫–∞
    table_data.append(['Effect Size', 'Value', 'Interpretation', ''])
    
    for effect_name, effect_data in effect_sizes.items():
        formatted_name = effect_name.replace('_', ' ').title()
        interpretation = effect_data.get('interpretation', '‚Äî')
        table_data.append([
            formatted_name,
            f"{effect_data['value']:.4f}",
            interpretation.title(),
            ''
        ])
    
    # –°–æ–∑–¥–∞–µ–º —Ç–∞–±–ª–∏—Ü—É
    table = ax7.table(cellText=table_data[1:], colLabels=table_data[0],
                     cellLoc='center', loc='center',
                     colWidths=[0.25, 0.25, 0.25, 0.25])
    
    table.auto_set_font_size(False)
    table.set_fontsize(10)
    table.scale(1.2, 1.5)
    
    # –†–∞—Å–∫—Ä–∞—à–∏–≤–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏
    for i in range(len(table_data[0])):
        table[(0, i)].set_facecolor('#40466e')
        table[(0, i)].set_text_props(weight='bold', color='white')
    
    # –†–∞—Å–∫—Ä–∞—à–∏–≤–∞–µ–º —Å—Ç—Ä–æ–∫–∏
    for i in range(1, len(table_data)):
        for j in range(len(table_data[0])):
            if i % 2 == 0:
                table[(i, j)].set_facecolor('#f1f1f2')
    
    plt.suptitle(f'üìä Comprehensive Statistical Analysis: {class_name}\n' +
                f'Overall Result: {"SIGNIFICANT" if statistical_results["overall_conclusion"]["significant"] else "NOT SIGNIFICANT"}',
                fontsize=18, y=0.98, weight='bold')
    
    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight', facecolor='white')
    
    plt.show()


print("‚úÖ –°–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ –≥–æ—Ç–æ–≤—ã!")
print("üé® –í–∫–ª—é—á–∞—é—Ç: XAI —à–∞–≥–∏, –∏–Ω—Ç–µ—Ä–≤–µ–Ω—Ü–∏–∏, Time-SHAP, —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑")
print("üìä –í—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–∞—è –≥—Ä–∞—Ñ–∏–∫–∞ —Å –¥–µ—Ç–∞–ª—å–Ω—ã–º–∏ –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è–º–∏")


def run_comprehensive_xai_pipeline(trajectory, timesteps, xai_analyzer, classifier, 
                                 target_class_id, target_class_name,
                                 save_results=True, results_dir=None):
    """
    –ó–∞–ø—É—Å–∫–∞–µ—Ç –ø–æ–ª–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω XAI –∞–Ω–∞–ª–∏–∑–∞
    
    –≠—Ç–æ—Ç –ø–∞–π–ø–ª–∞–π–Ω –≤—ã–ø–æ–ª–Ω—è–µ—Ç –≤—Å–µ –æ—Å–Ω–æ–≤–Ω—ã–µ —ç—Ç–∞–ø—ã XAI –∞–Ω–∞–ª–∏–∑–∞:
    1. –í—ã—á–∏—Å–ª–µ–Ω–∏–µ XAI –∫–∞—Ä—Ç –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ —à–∞–≥–∞
    2. –í—ã–¥–µ–ª–µ–Ω–∏–µ —Ç–æ–ø-k –∏ bottom-k —Ä–µ–≥–∏–æ–Ω–æ–≤
    3. –ö–æ–Ω—Ç—Ä–∞—Ñ–∞–∫—Ç—É–∞–ª—å–Ω—ã–µ –∏–Ω—Ç–µ—Ä–≤–µ–Ω—Ü–∏–∏
    4. –í—ã—á–∏—Å–ª–µ–Ω–∏–µ CFI –º–µ—Ç—Ä–∏–∫
    5. –°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è
    6. Time-SHAP –∞–Ω–∞–ª–∏–∑
    7. Sanity checks
    8. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á—ë—Ç–æ–≤
    
    Args:
        trajectory: —Å–ø–∏—Å–æ–∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω–æ–π —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏
        timesteps: —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —à–∞–≥–∏
        xai_analyzer: –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π XAI –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä
        classifier: –º–æ–¥–µ–ª—å –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞
        target_class_id: ID —Ü–µ–ª–µ–≤–æ–≥–æ –∫–ª–∞—Å—Å–∞
        target_class_name: –Ω–∞–∑–≤–∞–Ω–∏–µ —Ü–µ–ª–µ–≤–æ–≥–æ –∫–ª–∞—Å—Å–∞
        save_results: —Å–æ—Ö—Ä–∞–Ω—è—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        results_dir: –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
    
    Returns:
        dict: –ø–æ–ª–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞
    """
    
    print("üöÄ === –ó–ê–ü–£–°–ö –ö–û–ú–ü–õ–ï–ö–°–ù–û–ì–û XAI –ü–ê–ô–ü–õ–ê–ô–ù–ê ===")
    print(f"üéØ –¶–µ–ª–µ–≤–æ–π –∫–ª–∞—Å—Å: {target_class_name} (ID: {target_class_id})")
    print(f"üìà –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º—ã—Ö –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —à–∞–≥–æ–≤: {len(trajectory)}")
    print(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {save_results}")
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    if save_results and results_dir is None:
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        results_dir = RESULTS_DIR / f"xai_analysis_{target_class_name}_{timestamp}"
        results_dir.mkdir(parents=True, exist_ok=True)
        print(f"üìÅ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –±—É–¥—É—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {results_dir}")
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä—ã —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    results = {
        'metadata': {
            'target_class_id': target_class_id,
            'target_class_name': target_class_name,
            'n_timesteps': len(trajectory),
            'timesteps': timesteps,
            'analysis_timestamp': datetime.now().isoformat(),
            'parameters': {
                'top_k_percent': TOP_K_PERCENT,
                'bottom_k_percent': BOTTOM_K_PERCENT,
                'ig_n_steps': IG_N_STEPS,
                'shap_n_samples': SHAP_N_SAMPLES,
                'intervention_types': INTERVENTION_TYPES,
                'alpha_level': ALPHA_LEVEL
            }
        },
        'xai_maps': {},
        'region_analysis': {},
        'interventions': {},
        'cfi_analysis': {},
        'time_shap': {},
        'statistical_validation': {},
        'sanity_checks': {},
        'visualizations': []
    }
    
    try:
        # === –≠–¢–ê–ü 1: –í–´–ß–ò–°–õ–ï–ù–ò–ï XAI –ö–ê–†–¢ ===
        print("üî¨ –≠—Ç–∞–ø 1: –í—ã—á–∏—Å–ª–µ–Ω–∏–µ XAI –∫–∞—Ä—Ç –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ —à–∞–≥–∞...")
        
        xai_maps = {}
        region_data = {}
        
        total_frames = len(trajectory)
        for i, (image_tensor, timestep) in enumerate(tqdm(zip(trajectory, timesteps), 
                                                         desc="Computing XAI maps", 
                                                         total=total_frames)):
            image_gpu = image_tensor.to(device)
            
            try:
                # –°—á–∏—Ç–∞–µ–º –æ—Ç–¥–µ–ª—å–Ω–æ IG –∏ SHAP, —á—Ç–æ–±—ã —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –∏ –ø–æ–¥–ø–∏—Å–∞—Ç—å –∫–∞–∂–¥—É—é –∫–∞—Ä—Ç—É
                ig_attr = xai_analyzer.compute_integrated_gradients(image_gpu, target_class_id)
                shap_attr = xai_analyzer.compute_shap_approximation(image_gpu, target_class_id)
                combined_attr, method_details = xai_analyzer.compute_combined_attribution(
                    image_gpu, target_class_id, methods=['ig', 'shap'], weights=[0.5, 0.5]
                )
                
                # –í—ã–¥–µ–ª–µ–Ω–∏–µ —Ä–µ–≥–∏–æ–Ω–æ–≤
                top_k_data = select_regions_advanced(
                    combined_attr, k_percent=TOP_K_PERCENT, region_type='top'
                )
                
                bottom_k_data = select_regions_advanced(
                    combined_attr, k_percent=BOTTOM_K_PERCENT, region_type='bottom'
                )
                
                # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
                step_key = f"t_{timestep:.0f}"
                xai_maps[step_key] = {
                    'timestep': timestep,
                    'attribution_map': combined_attr,
                    'method_details': method_details,
                    'image_shape': tuple(image_tensor.shape)
                }
                
                region_data[step_key] = {
                    'top_k': top_k_data,
                    'bottom_k': bottom_k_data
                }
                
                # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –∫–∞–∂–¥–æ–≥–æ —à–∞–≥–∞
                if save_results:
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—É—é –∫–∞—Ä—Ç—É
                    viz_path = results_dir / f"xai_step_{step_key}.png"
                    visualize_xai_step_comprehensive(
                        image_tensor, combined_attr, 
                        top_k_data['mask'], bottom_k_data['mask'],
                        timestep, target_class_name, save_path=viz_path
                    )
                    results['visualizations'].append(str(viz_path))
                    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ —Å–æ—Ö—Ä–∞–Ω—è–µ–º IG –∏ SHAP –æ—Ç–¥–µ–ª—å–Ω–æ –¥–ª—è –ø—Ä–æ–∑—Ä–∞—á–Ω–æ—Å—Ç–∏
                    viz_path_ig = results_dir / f"xai_step_{step_key}_IG.png"
                    visualize_xai_step_comprehensive(
                        image_tensor, ig_attr,
                        top_k_data['mask'], bottom_k_data['mask'],
                        timestep, f"{target_class_name} (IG)", save_path=viz_path_ig
                    )
                    results['visualizations'].append(str(viz_path_ig))
                    viz_path_shap = results_dir / f"xai_step_{step_key}_SHAP.png"
                    visualize_xai_step_comprehensive(
                        image_tensor, shap_attr,
                        top_k_data['mask'], bottom_k_data['mask'],
                        timestep, f"{target_class_name} (SHAP)", save_path=viz_path_shap
                    )
                    results['visualizations'].append(str(viz_path_shap))
                else:
                    visualize_xai_step_comprehensive(
                        image_tensor, combined_attr, 
                        top_k_data['mask'], bottom_k_data['mask'],
                        timestep, target_class_name
                    )
                
            except Exception as e:
                print(f"   ‚ö†Ô∏è  –û—à–∏–±–∫–∞ –≤ —à–∞–≥–µ {i} (t={timestep}): {e}")
                continue
            # –õ–æ–≥–∏—Ä—É–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å XAI –ø–æ –∫–∞–¥—Ä–∞–º
            try:
                _log_progress_bar("XAI maps", i + 1, total_frames)
            except Exception:
                pass
        
        results['xai_maps'] = xai_maps
        results['region_analysis'] = region_data
        
        print(f"   ‚úÖ XAI –∫–∞—Ä—Ç—ã –≤—ã—á–∏—Å–ª–µ–Ω—ã –¥–ª—è {len(xai_maps)} —à–∞–≥–æ–≤")
        
        # === –≠–¢–ê–ü 2: –ö–û–ù–¢–†–ê–§–ê–ö–¢–£–ê–õ–¨–ù–´–ï –ò–ù–¢–ï–†–í–ï–ù–¶–ò–ò ===
        print("üß™ –≠—Ç–∞–ø 2: –ö–æ–Ω—Ç—Ä–∞—Ñ–∞–∫—Ç—É–∞–ª—å–Ω—ã–µ –∏–Ω—Ç–µ—Ä–≤–µ–Ω—Ü–∏–∏...")
        
        interventions_data = {}
        cfi_data = {}
        
        # –í—ã–±–∏—Ä–∞–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ –∫–ª—é—á–µ–≤—ã—Ö –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —à–∞–≥–æ–≤ –¥–ª—è –∏–Ω—Ç–µ—Ä–≤–µ–Ω—Ü–∏–π
        key_steps = [0, len(trajectory)//2, len(trajectory)-4,len(trajectory)-3,len(trajectory)-2, len(trajectory)-1]  # –ù–∞—á–∞–ª–æ, —Å–µ—Ä–µ–¥–∏–Ω–∞, –∫–æ–Ω–µ—Ü
        
        total_keys = len(key_steps)
        for idx_k, step_idx in enumerate(key_steps):
            if step_idx >= len(trajectory):
                continue
                
            image_tensor = trajectory[step_idx]
            timestep = timesteps[step_idx]
            step_key = f"t_{timestep:.0f}"
            
            if step_key not in region_data:
                continue
            
            print(f"   üî¨ –ê–Ω–∞–ª–∏–∑ –∏–Ω—Ç–µ—Ä–≤–µ–Ω—Ü–∏–π –¥–ª—è t={timestep:.0f}...")
            
            image_gpu = image_tensor.to(device)
            step_interventions = {}
            step_cfi = {}
            
            for region_type in ['top_k', 'bottom_k']:
                region_mask = region_data[step_key][region_type]['mask']
                step_interventions[region_type] = {}
                
                for intervention_type in INTERVENTION_TYPES:
                    try:
                        # –ò–Ω—Ç–µ—Ä–≤–µ–Ω—Ü–∏—è
                        intervention_result = counterfactual_intervention_advanced(
                            image_gpu, region_mask, intervention_type
                        )
                        
                        # CFI –∞–Ω–∞–ª–∏–∑
                        cfi_result = compute_causal_shift_comprehensive(
                            classifier, image_gpu, 
                            intervention_result['modified_image'],
                            target_class_id, include_all_classes=True
                        )
                        
                        step_interventions[region_type][intervention_type] = intervention_result
                        step_cfi[f"{region_type}_{intervention_type}"] = cfi_result
                        
                    except Exception as e:
                        print(f"      ‚ö†Ô∏è  –û—à–∏–±–∫–∞ –≤ {region_type}/{intervention_type}: {e}")
                        continue
            
            interventions_data[step_key] = step_interventions
            cfi_data[step_key] = step_cfi
            
            # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –∏–Ω—Ç–µ—Ä–≤–µ–Ω—Ü–∏–π
            if save_results:
                viz_path = results_dir / f"interventions_{step_key}.png"
                visualize_intervention_comprehensive(
                    image_tensor, 
                    {region_type: region_data[step_key][region_type]['mask'] 
                     for region_type in ['top_k', 'bottom_k']},
                    step_interventions, step_cfi, timestep, save_path=viz_path
                )
                results['visualizations'].append(str(viz_path))
            # –õ–æ–≥–∏—Ä—É–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å –ø–æ –∏–Ω—Ç–µ—Ä–≤–µ–Ω—Ü–∏—è–º/CFI
            try:
                _log_progress_bar("Interventions/CFI", idx_k + 1, total_keys)
            except Exception:
                pass
        
        results['interventions'] = interventions_data
        results['cfi_analysis'] = cfi_data
        
        print(f"   ‚úÖ –ò–Ω—Ç–µ—Ä–≤–µ–Ω—Ü–∏–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω—ã –¥–ª—è {len(interventions_data)} –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —à–∞–≥–æ–≤")
        
        # === –≠–¢–ê–ü 3: TIME-SHAP –ê–ù–ê–õ–ò–ó ===
        print("üïí –≠—Ç–∞–ø 3: Time-SHAP –∞–Ω–∞–ª–∏–∑ –≤—Ä–µ–º–µ–Ω–Ω–æ–π –≤–∞–∂–Ω–æ—Å—Ç–∏...")
        
        try:
            time_importance, time_data = xai_analyzer.compute_time_shap(
                trajectory, timesteps, target_class_id
            )
            
            results['time_shap'] = {
                'importance': time_importance,
                'raw_data': time_data,
                'most_important_timestep': timesteps[np.argmax(time_importance)],
                'most_important_index': int(np.argmax(time_importance))
            }
            
            # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è Time-SHAP
            if save_results:
                viz_path = results_dir / "time_shap_analysis.png"
                plot_time_shap_comprehensive(
                    timesteps, time_importance, time_data, 
                    target_class_name, save_path=viz_path
                )
                results['visualizations'].append(str(viz_path))
            else:
                plot_time_shap_comprehensive(
                    timesteps, time_importance, time_data, target_class_name
                )
            
            print("   ‚úÖ Time-SHAP –∞–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à—ë–Ω")
            try:
                _log_progress_bar("Time-SHAP", 1, 1)
            except Exception:
                pass
            
        
            

        except Exception as e:
            print(f"   ‚ùå –û—à–∏–±–∫–∞ –≤ Time-SHAP –∞–Ω–∞–ª–∏–∑–µ: {e}")
            results['time_shap'] = {'error': str(e)}



        device_torch = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        try:
            # –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –∏ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–µ–π
            print("üîç –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–µ–π...")
            
            target_layer = classifier.model.layer4[-1].conv2
            print(f"‚úÖ Target layer: {target_layer}")

            gradcam_results = {}
            all_cams = []

            # –°–æ–∑–¥–∞—ë–º –æ–±–æ–ª–æ—á–∫—É –±–µ–∑ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏
            class RawClassifier(torch.nn.Module):
                def __init__(self, original_classifier):
                    super().__init__()
                    self.model = original_classifier.model
                    
                def forward(self, x):
                    return self.model(x)
            
            raw_classifier = RawClassifier(classifier).to(device_torch)
            raw_classifier.eval()
            
            def manual_preprocess(image_tensor):
                """–†—É—á–Ω–∞—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º–∏ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—è–º–∏"""
                # –ü—Ä–∏–≤–æ–¥–∏–º –∫ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–π 4D —Ñ–æ—Ä–º–µ [N, C, H, W]
                if len(image_tensor.shape) == 5:  # [1, 1, 3, 128, 128]
                    x = image_tensor.squeeze(1)  # [1, 3, 128, 128]
                elif len(image_tensor.shape) == 3:  # [3, 128, 128]
                    x = image_tensor.unsqueeze(0)  # [1, 3, 128, 128]
                else:
                    x = image_tensor
                
                # –£–±–µ–∂–¥–∞–µ–º—Å—è —á—Ç–æ —ç—Ç–æ [1, 3, H, W]
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å (–±–µ–∑ –≤—ã–≤–æ–¥–∞ –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è)
                if x.shape == 1 and x.shape == 3: pass

                    
                # –ò–∑ [-1,1] –≤ [0,1]
                x = torch.clamp((x + 1.0) / 2.0, 0, 1)
                
                # –†–µ—Å–∞–π–∑ –¥–æ 224x224
                x = F.interpolate(x, size=(224, 224), mode='bilinear', align_corners=False, antialias=True)
                
                # ImageNet –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
                mean = torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1).to(device_torch)
                std = torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1).to(device_torch)
                x = (x - mean) / std
                
                return x
            
            with GradCAM(model=raw_classifier, target_layers=[target_layer]) as cam:
                for i, (image_tensor, timestep) in enumerate(zip(trajectory, timesteps)):
                    # –ò—Å–ø—Ä–∞–≤–ª—è–µ–º —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏
                    #print(f"üìê –ò—Å—Ö–æ–¥–Ω–∞—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å image_tensor: {image_tensor.shape}")
                    
                    # –ü—Ä–∏–≤–æ–¥–∏–º –∫ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ [3, 128, 128]
                    if len(image_tensor.shape) == 4 and image_tensor.shape[0] == 1:  # [1, 3, 128, 128]
                        clean_tensor = image_tensor.squeeze(0)  # [3, 128, 128]
                    elif len(image_tensor.shape) == 5:  # [1, 1, 3, 128, 128]
                        clean_tensor = image_tensor.squeeze(0).squeeze(0)  # [3, 128, 128]
                    else:
                        clean_tensor = image_tensor  # —É–∂–µ [3, 128, 128]
                    
                    #print(f"üìê –û—á–∏—â–µ–Ω–Ω–∞—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å: {clean_tensor.shape}")
                    
                    # –¢–µ–ø–µ—Ä—å –¥–æ–±–∞–≤–ª—è–µ–º batch dimension –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
                    raw_input = clean_tensor.unsqueeze(0).to(device_torch)  # [1, 3, 128, 128]
                    processed_input = manual_preprocess(raw_input)  # [1, 3, 224, 224]
                    
                    #print(f"üìê –ü–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {processed_input.shape}")
                    
                    # GradCAM
                    grayscale_cam = cam(
                        input_tensor=processed_input,
                        targets=[ClassifierOutputTarget(target_class_id)]
                    )
                    grayscale_cam = grayscale_cam[0, :]  # (224,224)
                    all_cams.append(grayscale_cam)

                    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
                    rgb_img = clean_tensor.permute(1, 2, 0).detach().cpu().numpy().astype(np.float32)  # (128,128,3)
                    rgb_img = (rgb_img + 1.0) / 2.0  # [-1,1] -> [0,1]
                    rgb_img = np.clip(rgb_img, 0, 1)
                    
                    # –†–µ—Å–∞–π–∑ –¥–æ 224x224 –¥–ª—è —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è —Å CAM
                    from skimage.transform import resize
                    rgb_img_224 = resize(rgb_img, (224, 224), anti_aliasing=True, preserve_range=False)
                    
                    cam_image = show_cam_on_image(rgb_img_224, grayscale_cam, use_rgb=True)

                    step_key = f"t_{timestep:.0f}"
                    gradcam_results[step_key] = grayscale_cam

                    if save_results:
                        cam_path = results_dir / f"gradcam_{step_key}.png"
                        plt.imsave(cam_path, cam_image)
                        results['visualizations'].append(str(cam_path))
                    else:
                        plt.figure(figsize=(8,4))
                        plt.subplot(1,2,1)
                        plt.imshow(rgb_img_224)
                        plt.title(f"–ò—Å—Ö–æ–¥–Ω–æ–µ t={timestep:.0f}")
                        plt.axis('off')
                        plt.subplot(1,2,2)
                        plt.imshow(cam_image)
                        plt.title(f"Grad-CAM t={timestep:.0f}")
                        plt.axis('off')
                        plt.tight_layout()
                        plt.show()

            results['gradcam'] = gradcam_results
            print("   ‚úÖ Grad-CAM –ø–æ —à–∞–≥–∞–º –≥–æ—Ç–æ–≤")

            # Grad-CAM –¥–ª—è –≤–∞–∂–Ω–æ–≥–æ —à–∞–≥–∞
            if 'time_shap' in results and 'most_important_index' in results['time_shap']:
                imp_idx = results['time_shap']['most_important_index']
                imp_timestep = timesteps[imp_idx]
                
                # –ò—Å–ø—Ä–∞–≤–ª—è–µ–º —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ –¥–ª—è –≤–∞–∂–Ω–æ–≥–æ —à–∞–≥–∞
                imp_tensor = trajectory[imp_idx]
                if len(imp_tensor.shape) == 4 and imp_tensor.shape[0] == 1:
                    imp_clean = imp_tensor.squeeze(0)
                elif len(imp_tensor.shape) == 5:
                    imp_clean = imp_tensor.squeeze(0).squeeze(0)
                else:
                    imp_clean = imp_tensor
                
                with GradCAM(model=raw_classifier, target_layers=[target_layer]) as cam:
                    imp_raw = imp_clean.unsqueeze(0).to(device_torch)
                    imp_processed = manual_preprocess(imp_raw)
                    
                    grayscale_cam = cam(
                        input_tensor=imp_processed,
                        targets=[ClassifierOutputTarget(target_class_id)]
                    )
                
                grayscale_cam = grayscale_cam[0, :]
                
                rgb_img = imp_clean.permute(1, 2, 0).detach().cpu().numpy().astype(np.float32)
                rgb_img = (rgb_img + 1.0) / 2.0
                rgb_img = np.clip(rgb_img, 0, 1)
                rgb_img_224 = resize(rgb_img, (224, 224), anti_aliasing=True, preserve_range=False)
                
                cam_image = show_cam_on_image(rgb_img_224, grayscale_cam, use_rgb=True)

                if save_results:
                    imp_path = results_dir / f"gradcam_most_important_t{imp_timestep:.0f}.png"
                    plt.imsave(imp_path, cam_image)
                    results['visualizations'].append(str(imp_path))
                else:
                    plt.imshow(cam_image)
                    plt.title(f"Grad-CAM –≤–∞–∂–Ω—ã–π —à–∞–≥ t={imp_timestep:.0f}")
                    plt.axis('off')
                    plt.show()

                results['gradcam_most_important'] = {
                    'timestep': float(imp_timestep),
                    'index': int(imp_idx),
                    'gradcam': grayscale_cam
                }
                print(f"   ‚úÖ Grad-CAM –¥–ª—è –≤–∞–∂–Ω–æ–≥–æ —à–∞–≥–∞ (t={imp_timestep:.0f}) –≥–æ—Ç–æ–≤")

            # –°—É–º–º–∞—Ä–Ω—ã–π CAM
            if len(all_cams) > 0:
                summed_cam = np.mean(np.stack(all_cams, axis=0), axis=0)
                summed_cam = (summed_cam - summed_cam.min()) / (summed_cam.max() - summed_cam.min() + 1e-8)

                final_tensor = trajectory[-1]
                if len(final_tensor.shape) == 4 and final_tensor.shape[0] == 1:
                    final_clean = final_tensor.squeeze(0)
                elif len(final_tensor.shape) == 5:
                    final_clean = final_tensor.squeeze(0).squeeze(0)
                else:
                    final_clean = final_tensor

                final_img = final_clean.permute(1, 2, 0).detach().cpu().numpy().astype(np.float32)
                final_img = (final_img + 1.0) / 2.0
                final_img = np.clip(final_img, 0, 1)
                final_img_224 = resize(final_img, (224, 224), anti_aliasing=True, preserve_range=False)
                
                cam_image = show_cam_on_image(final_img_224, summed_cam, use_rgb=True)

                if save_results:
                    sum_path = results_dir / "gradcam_summary_all_timesteps.png"
                    plt.imsave(sum_path, cam_image)
                    results['visualizations'].append(str(sum_path))
                else:
                    plt.imshow(cam_image)
                    plt.title("–°—É–º–º–∞—Ä–Ω—ã–π Grad-CAM –ø–æ –≤—Å–µ–º t")
                    plt.axis('off')
                    plt.show()

                results['gradcam_summary'] = summed_cam
                print("   ‚úÖ –°—É–º–º–∞—Ä–Ω—ã–π Grad-CAM —Ä–∞—Å—Å—á–∏—Ç–∞–Ω")
                print("gradcam_overview save")
                # –ò—Å—Ö–æ–¥–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
                plt.figure(figsize=(16,5))

                # 1. –û—Ä–∏–≥–∏–Ω–∞–ª
                plt.subplot(1,3,1)
                plt.imshow(final_img_224)
                plt.title("Original")
                plt.axis('off')

                # 2. –í–∞–∂–Ω—ã–π Grad-CAM
                plt.subplot(1,3,2)
                plt.imshow(show_cam_on_image(final_img_224, results['gradcam_most_important']['gradcam'], use_rgb=True))
                plt.title("Most important Grad-CAM (t={:.0f})".format(results['gradcam_most_important']['timestep']))
                plt.axis('off')

                # 3. –°—É–º–º–∞—Ä–Ω—ã–π Grad-CAM
                plt.subplot(1,3,3)
                plt.imshow(show_cam_on_image(final_img_224, results['gradcam_summary'], use_rgb=True))
                plt.title("Summed Grad-CAM")
                plt.axis('off')

                plt.tight_layout()
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –µ–¥–∏–Ω—ã–π –∫–æ–ª–ª–∞–∂
                plt.savefig(results_dir / "gradcam_overview.png")
                plt.close()
            try:
                _log_progress_bar("Grad-CAM", 1, 1)
            except Exception:
                pass
        except Exception as e:
            print(f"   ‚ùå –û—à–∏–±–∫–∞ –≤ Grad-CAM: {e}")
            import traceback
            traceback.print_exc()
            results['gradcam'] = {'error': str(e)}




        # === –≠–¢–ê–ü 4: –°–ë–û–† CFI –î–ê–ù–ù–´–• –î–õ–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ò ===
        print("üìä –≠—Ç–∞–ø 4: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞...")
        
        top_k_shifts = []
        bottom_k_shifts = []
        
        for step_key, step_cfi in cfi_data.items():
            for intervention_key, cfi_result in step_cfi.items():
                if 'top_k' in intervention_key:
                    top_k_shifts.append(cfi_result['target_class_analysis']['cfi'])
                elif 'bottom_k' in intervention_key:
                    bottom_k_shifts.append(cfi_result['target_class_analysis']['cfi'])
        
        print(f"   üìà –°–æ–±—Ä–∞–Ω–æ CFI –∑–Ω–∞—á–µ–Ω–∏–π: Top-k={len(top_k_shifts)}, Bottom-k={len(bottom_k_shifts)}")
        
        # === –≠–¢–ê–ü 5: –°–¢–ê–¢–ò–°–¢–ò–ß–ï–°–ö–ê–Ø –í–ê–õ–ò–î–ê–¶–ò–Ø ===
        print("üìä –≠—Ç–∞–ø 5: –ö–æ–º–ø–ª–µ–∫—Å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è...")
        
        if len(top_k_shifts) > 0 and len(bottom_k_shifts) > 0:
            try:
                statistical_results = statistical_validation_comprehensive(
                    top_k_shifts, bottom_k_shifts,
                    alpha=ALPHA_LEVEL, 
                    n_bootstrap=N_BOOTSTRAP,
                    n_permutations=N_PERMUTATIONS
                )
                
                results['statistical_validation'] = statistical_results
                
                # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
                if save_results:
                    viz_path = results_dir / "statistical_analysis.png"
                    plot_statistical_analysis_modern(
                        statistical_results, target_class_name, save_path=viz_path
                    )
                    results['visualizations'].append(str(viz_path))
                else:
                    plot_statistical_analysis_modern(statistical_results, target_class_name)
                
                print(f"   ‚úÖ –°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
                
            except Exception as e:
                print(f"   ‚ùå –û—à–∏–±–∫–∞ –≤ —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–æ–π –≤–∞–ª–∏–¥–∞—Ü–∏–∏: {e}")
                results['statistical_validation'] = {'error': str(e)}
        else:
            print("   ‚ö†Ô∏è  –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–æ–π –≤–∞–ª–∏–¥–∞—Ü–∏–∏")
            results['statistical_validation'] = {'error': 'Insufficient data'}
        
        # === –≠–¢–ê–ü 6: SANITY CHECKS ===
        print("üîç –≠—Ç–∞–ø 6: Sanity checks...")
        
        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ñ–∏–Ω–∞–ª—å–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –¥–ª—è sanity check
            final_image = trajectory[-1].to(device)
            
            sanity_results = sanity_check_comprehensive(
                classifier, final_image, target_class_id, xai_analyzer,
                n_trials=3, randomization_strength=0.01
            )
            
            results['sanity_checks'] = sanity_results
            print("   ‚úÖ Sanity checks –∑–∞–≤–µ—Ä—à–µ–Ω—ã")
            
        except Exception as e:
            print(f"   ‚ùå –û—à–∏–±–∫–∞ –≤ sanity checks: {e}")
            results['sanity_checks'] = {'error': str(e)}
        
        # === –≠–¢–ê–ü 7: –°–û–•–†–ê–ù–ï–ù–ò–ï –†–ï–ó–£–õ–¨–¢–ê–¢–û–í ===
        if save_results:
            print("üíæ –≠—Ç–∞–ø 7: –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤...")
            
            # JSON –æ—Ç—á—ë—Ç
            json_path = results_dir / 'analysis_results.json'
            
            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è JSON (—É–±–∏—Ä–∞–µ–º —Ç–µ–Ω–∑–æ—Ä—ã)
            json_results = results.copy()
            
            # –£–±–∏—Ä–∞–µ–º —Ç–µ–Ω–∑–æ—Ä—ã –∏–∑ xai_maps
            for step_key in json_results.get('xai_maps', {}):
                if 'attribution_map' in json_results['xai_maps'][step_key]:
                    del json_results['xai_maps'][step_key]['attribution_map']
            
            # –£–±–∏—Ä–∞–µ–º —Ç–µ–Ω–∑–æ—Ä—ã –∏–∑ –∏–Ω—Ç–µ—Ä–≤–µ–Ω—Ü–∏–π
            for step_key in json_results.get('interventions', {}):
                for region_type in json_results['interventions'][step_key]:
                    for intervention_type in json_results['interventions'][step_key][region_type]:
                        intervention_data = json_results['interventions'][step_key][region_type][intervention_type]
                        for key in ['modified_image', 'intervention', 'mask_tensor', 'difference']:
                            if key in intervention_data:
                                del intervention_data[key]
            
            with open(json_path, 'w', encoding='utf-8') as f:
                json.dump(json_results, f, indent=2, ensure_ascii=False, default=str)
            
            # Pickle –¥–ª—è –ø–æ–ª–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
            pickle_path = results_dir / 'full_results.pkl'
            with open(pickle_path, 'wb') as f:
                pickle.dump(results, f)
            
            print(f"   üìÑ JSON –æ—Ç—á—ë—Ç: {json_path}")
            print(f"   üóÇÔ∏è –ü–æ–ª–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ: {pickle_path}")
            print(f"   üé® –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏: {len(results['visualizations'])} —Ñ–∞–π–ª–æ–≤")
        
        # === –ò–¢–û–ì–û–í–´–ô –û–¢–ß–Å–¢ ===
        print("üéâ === –ê–ù–ê–õ–ò–ó –ó–ê–í–ï–†–®–Å–ù ===")
        print(f"üéØ –ö–ª–∞—Å—Å: {target_class_name}")
        print(f"üìä XAI –∫–∞—Ä—Ç: {len(results['xai_maps'])}")
        print(f"üß™ –ò–Ω—Ç–µ—Ä–≤–µ–Ω—Ü–∏–π: {sum(len(step_data) for step_data in results['interventions'].values())}")
        
        if 'statistical_validation' in results and 'overall_conclusion' in results['statistical_validation']:
            conclusion = results['statistical_validation']['overall_conclusion']
            print(f"üìà –°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∞—è –∑–Ω–∞—á–∏–º–æ—Å—Ç—å: {'‚úÖ –î–ê' if conclusion['significant'] else '‚ùå –ù–ï–¢'}")
        
        if 'sanity_checks' in results and 'overall_sanity_score' in results['sanity_checks']:
            sanity_score = results['sanity_checks']['overall_sanity_score']
            print(f"üîç Sanity score: {sanity_score:.2f}/1.0")
        
        return results
        
    except Exception as e:
        print(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –≤ –ø–∞–π–ø–ª–∞–π–Ω–µ: {e}")
        results['pipeline_error'] = str(e)
        return results


# === –ó–ê–ü–£–°–ö –û–°–ù–û–í–ù–û–ì–û –ü–ê–ô–ü–õ–ê–ô–ù–ê ===

if XAI_ANALYZER_READY:
    print("üöÄ === –ì–û–¢–û–í –ö –ó–ê–ü–£–°–ö–£ –û–°–ù–û–í–ù–û–ì–û –ü–ê–ô–ü–õ–ê–ô–ù–ê ===")
    
    # –°–ø—Ä–∞—à–∏–≤–∞–µ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –æ –∑–∞–ø—É—Å–∫–µ
    print(f"üéØ –¶–µ–ª–µ–≤–æ–π –∫–ª–∞—Å—Å: {TARGET_CLASS_NAME}")
    print(f"üìà –í—Ä–µ–º–µ–Ω–Ω—ã—Ö —à–∞–≥–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞: {len(timesteps)}")
    print(f"‚öôÔ∏è  –ò–Ω—Ç–µ—Ä–≤–µ–Ω—Ü–∏–∏: {', '.join(INTERVENTION_TYPES)}")
    print(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞: Œ±={ALPHA_LEVEL}, bootstrap={N_BOOTSTRAP}, permutations={N_PERMUTATIONS}")
    print()
    
    # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –∑–∞–ø—É—Å–∫ (—Ä–∞—Å–∫–æ–º–º–µ–Ω—Ç–∏—Ä—É–π—Ç–µ –¥–ª—è —Ä—É—á–Ω–æ–≥–æ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è)
    # user_input = input("ü§î –ó–∞–ø—É—Å—Ç–∏—Ç—å –ø–æ–ª–Ω—ã–π XAI –∞–Ω–∞–ª–∏–∑? (y/n): ")
    # if user_input.lower() in ['y', 'yes', '–¥–∞']:
    
    if True:  # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –∑–∞–ø—É—Å–∫
        print("üé¨ –ó–∞–ø—É—Å–∫–∞–µ–º –ø–æ–ª–Ω—ã–π XAI –ø–∞–π–ø–ª–∞–π–Ω...")
        # –ó–∞–ø—É—Å–∫ –ø–∞–π–ø–ª–∞–π–Ω–∞
        final_results = run_comprehensive_xai_pipeline(
            trajectory=trajectory,
            timesteps=timesteps,
            xai_analyzer=xai_analyzer,
            classifier=classifier,
            target_class_id=TARGET_CLASS_ID,
            target_class_name=TARGET_CLASS_NAME,
            save_results=True
        )
        
        print("üèÅ === –ü–ê–ô–ü–õ–ê–ô–ù –ó–ê–í–ï–†–®–Å–ù ===")
        
        # –ö—Ä–∞—Ç–∫–æ–µ —Ä–µ–∑—é–º–µ
        if 'pipeline_error' not in final_results:
            print("‚úÖ –ü–∞–π–ø–ª–∞–π–Ω –≤—ã–ø–æ–ª–Ω–µ–Ω —É—Å–ø–µ—à–Ω–æ!")
            
            if 'statistical_validation' in final_results and 'overall_conclusion' in final_results['statistical_validation']:
                stats_result = final_results['statistical_validation']['overall_conclusion']
                if stats_result['significant']:
                    print(f"üéâ –†–ï–ó–£–õ–¨–¢–ê–¢: –û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏ –∑–Ω–∞—á–∏–º–∞—è —Ä–∞–∑–Ω–∏—Ü–∞ –º–µ–∂–¥—É Top-k –∏ Bottom-k —Ä–µ–≥–∏–æ–Ω–∞–º–∏!")
                    print(f"   –≠—Ç–æ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–∞–µ—Ç, —á—Ç–æ XAI –º–µ—Ç–æ–¥—ã –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ –≤—ã–¥–µ–ª—è—é—Ç –∫–∞—É–∑–∞–ª—å–Ω–æ –≤–∞–∂–Ω—ã–µ —Ä–µ–≥–∏–æ–Ω—ã.")
                else:
                    print(f"‚ö†Ô∏è  –†–ï–ó–£–õ–¨–¢–ê–¢: –°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏ –∑–Ω–∞—á–∏–º–∞—è —Ä–∞–∑–Ω–∏—Ü–∞ –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∞.")
                    print(f"   –í–æ–∑–º–æ–∂–Ω–æ, —Ç—Ä–µ–±—É–µ—Ç—Å—è –±–æ–ª—å—à–µ –¥–∞–Ω–Ω—ã—Ö –∏–ª–∏ –¥—Ä—É–≥–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∞–Ω–∞–ª–∏–∑–∞.")
            
            print(f"üìÅ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {RESULTS_DIR}")
            
        else:
            print(f"‚ùå –ü–∞–π–ø–ª–∞–π–Ω –∑–∞–≤–µ—Ä—à–∏–ª—Å—è —Å –æ—à–∏–±–∫–æ–π: {final_results['pipeline_error']}")
    
    else:
        print("‚è∏Ô∏è  –ü–∞–π–ø–ª–∞–π–Ω –æ—Ç–º–µ–Ω—ë–Ω –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º.")
        print("üí° –í—ã –º–æ–∂–µ—Ç–µ –∑–∞–ø—É—Å—Ç–∏—Ç—å –æ—Ç–¥–µ–ª—å–Ω—ã–µ —á–∞—Å—Ç–∏ –∞–Ω–∞–ª–∏–∑–∞ —Å –ø–æ–º–æ—â—å—é —Ñ—É–Ω–∫—Ü–∏–π –≤—ã—à–µ.")

else:
    print("‚ùå XAI –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –Ω–µ –≥–æ—Ç–æ–≤. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ø—Ä–µ–¥—ã–¥—É—â–∏–µ —à–∞–≥–∏.")
    print("üîß –£–±–µ–¥–∏—Ç–µ—Å—å —á—Ç–æ –º–æ–¥–µ–ª–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –∏ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏—è —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–∞.")


